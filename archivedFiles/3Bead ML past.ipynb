{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "17d34f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "66f3121e-b0e4-46b0-ab2a-09540cec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b280ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def EXIT_NOTEBOOK(): os._exit(00)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "36ba82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "\n",
    "# # ps\n",
    "# import pysindy as ps\n",
    "\n",
    "# sns.set_theme()\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a9a3b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 6\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339b9d",
   "metadata": {},
   "source": [
    "# Dataset for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "818019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 11)\n",
      "[[[-1.85958495e-02 -2.34382707e-02 -5.42650682e-03 -2.39646514e-02\n",
      "   -3.77401638e-02 -6.45540354e-02 -9.54499720e-02 -9.50182513e-02\n",
      "   -6.45613912e-02 -8.95568240e-02 -8.95568240e-02]\n",
      "  [-1.84951965e-02 -4.40069403e-02 -4.67088782e-02 -4.57419627e-02\n",
      "   -5.89102125e-02 -2.93702734e-02 -1.20002004e-02 -2.51019380e-02\n",
      "   -3.79777379e-02 -4.85348623e-02 -4.85348623e-02]]\n",
      "\n",
      " [[-1.11380522e-02 -2.28152367e-02 -3.56737015e-02 -2.46125531e-02\n",
      "   -2.72768969e-03 -2.03515907e-02 -2.40362412e-02 -3.89467749e-02\n",
      "   -3.81666208e-02 -2.38841518e-02 -2.38841518e-02]\n",
      "  [-4.84693304e-03 -7.65963959e-03 -1.04494194e-02 -1.40798705e-02\n",
      "   -8.88022471e-03 -2.46445275e-04  1.35308888e-02  9.90989935e-03\n",
      "   -2.03964478e-02 -3.86028364e-03 -3.86028364e-03]]\n",
      "\n",
      " [[-9.24629130e-03 -1.60675901e-02 -1.84644186e-02 -4.14103594e-03\n",
      "    1.41646522e-03 -3.66584557e-02 -3.86298577e-02 -1.99072503e-02\n",
      "   -1.61804482e-02 -1.06451244e-02 -1.06451244e-02]\n",
      "  [-3.47521240e-02 -7.79033292e-03 -8.01978745e-03  1.35879422e-02\n",
      "    2.86336943e-02  1.00047325e-02  4.34510365e-02  2.75247873e-03\n",
      "   -1.27211743e-03  1.69694812e-06  1.69694812e-06]]]\n"
     ]
    }
   ],
   "source": [
    "from BeadModel import Simulate\n",
    "from SimulationParameters import *\n",
    "X = Simulation(*Simulate(numSims)).positions[:,:,:,0]\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbd757",
   "metadata": {},
   "source": [
    "# Set the NN model and Solver with training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "606412e9",
   "metadata": {
    "code_folding": [
     2,
     18,
     19,
     28,
     42,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def relu2(X): return func.relu(X)**2\n",
    "def tanh(X): return func.tanh(X)\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=6,num_hidden=2,hidden_dim=10,act=func.tanh,transform=None):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers  = nn.ModuleList([nn.Linear(input_dim,hidden_dim)])\n",
    "        for _ in range(num_hidden-1): self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.act     = act\n",
    "        self.out     = nn.Linear(hidden_dim,output_dim)\n",
    "        self.transform = transform\n",
    "    def forward(self,X):\n",
    "        if self.transform is not None: X = self.transform(X)\n",
    "        for layer in self.layers: X = self.act(layer(X))\n",
    "        Y = self.out(X)\n",
    "        return Y\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,dim,model_U,unit_len=int(5e3)):\n",
    "        super().__init__()\n",
    "        self.dim      = dim\n",
    "        self.model_U  = model_U\n",
    "        self.unit_len = unit_len\n",
    "        self.mu       = nn.Parameter(torch.tensor([0.]*dim),requires_grad=False) \n",
    "        self.sigma    = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #above two lines should work, but if something doesn't work, check here! Does current self.mu and self.sigma code work if dim>1. Should return a vector since what we're trying to do is calculate mu\n",
    "        #and sigma separately for each imnputted feature. mu is parameter[0] for each row.\n",
    "        self.coef_U   = nn.Parameter(torch.tensor(1.)*dim,requires_grad=False)\n",
    "        #self.mu       = nn.Parameter(torch.tensor([0.]*dim).cuda(),requires_grad=False)\n",
    "        #self.sigma    = nn.Parameter(torch.tensor([1.]*dim).cuda(),requires_grad=False)\n",
    "        #self.coef_U   = nn.Parameter(torch.tensor(1.).cuda(),requires_grad=False)\n",
    "    def get_U_harmonic(self,X): return torch.sum(X**2,axis=-1)\n",
    "\n",
    "\n",
    "    def get_U_dU(self, X):\n",
    "        U_list = []\n",
    "        dU_list = []\n",
    "        for t in range(X.shape[-1]):\n",
    "            X_t = X[..., t]\n",
    "            X_t_ = torch.tensor(X_t, dtype=torch.float32, requires_grad=True)\n",
    "            # X_t_ = (X_t - self.mu) / self.sigma #normalization necessary, column wise is standard\n",
    "            # U_t = torch.sum(self.coef_U * (self.model_U(X_t_) + self.get_U_harmonic(X_t_))) #*certify validity\n",
    "            print()\n",
    "            print(self.coef_U)\n",
    "            print()\n",
    "            print(self.model_U(X_t_))\n",
    "            print()\n",
    "            print(self.get_U_harmonic(X_t_))\n",
    "            print()\n",
    "            U_t = self.coef_U * (self.model_U(X_t_).view(-1) + self.get_U_harmonic(X_t_))\n",
    "            U_list.append(U_t)\n",
    "            dU_t = torch.autograd.grad(\n",
    "                outputs=U_t,\n",
    "                inputs=X_t,\n",
    "                grad_outputs=torch.ones_like(U_t),\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "            dU_list.append(dU_t)\n",
    "        U = torch.stack(U_list, dim=0)\n",
    "        dU = torch.stack(dU_list, dim=-1)\n",
    "        return U, dU\n",
    "\n",
    "    \n",
    "    def get_U_np(self,X): \n",
    "        U,_ = self.get_U_dU(X);\n",
    "        return U.cpu().data.numpy()\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "    def train_model(self,data_train,data_test,get_loss,optimizer,\n",
    "                    n_steps,batch_size,scheduler=None,n_show_loss=100,error_model=None,use_tqdm=True):\n",
    "        if use_tqdm: step_range = tqdm(range(n_steps))\n",
    "        else: step_range = range(n_steps)\n",
    "        loss_step = []\n",
    "        for i_step in step_range:\n",
    "            if i_step%n_show_loss==0:\n",
    "                loss_train,loss_test = get_loss(self.model,data_train)[:-1],\\\n",
    "                                       get_loss(self.model,data_test)[:-1]\n",
    "                \n",
    "                def show_num(x): \n",
    "                    if abs(x)<100 and abs(x)>.01: return '%0.5f'%x\n",
    "                    else: return '%0.2e'%x\n",
    "                item1 = '%2dk'%np.int_(i_step/1000)\n",
    "                item2 = 'Loss: '+' '.join([show_num(k) for k in loss_train])\n",
    "                item3 = ' '.join([show_num(k) for k in loss_test])\n",
    "                item4 = ''\n",
    "                if error_model is not None: item4 = 'E(QP): %0.4f' % (error_model(self.model))\n",
    "                print(', '.join([item1,item2,item3,item4]))\n",
    "                loss_step = loss_step + [i_step] + [k.cpu().data.numpy() for k in loss_train]\\\n",
    "                                                 + [k.cpu().data.numpy() for k in loss_train]\n",
    "            data_batch = data_train[random.sample(range(len(data_train)),\n",
    "                                                  min(batch_size,len(data_train)))]\n",
    "#             print(i_step,data_batch.shape)\n",
    "            loss = get_loss(self.model,data_batch)[-1]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        if error_model is not None: \n",
    "            print(\"Error: %0.5f\" % (error_model(self.model)))\n",
    "        return loss_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5786a635",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fd4f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_U_dU(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "94e8e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.mu.shape)\n",
    "# print(model.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82dfe6",
   "metadata": {},
   "source": [
    "# Set the loss function and Train the model for differen a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "69c4eeaf",
   "metadata": {
    "code_folding": [
     0,
     12,
     16
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter containing:\n",
      "tensor(6.)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jf/w5m4c3sj361fdh8b5wz1cy1m0000gn/T/ipykernel_97939/3909905085.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_t_ = torch.tensor(X_t, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 6x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;66;03m#,loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# plot_model(model)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[280], line 22\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m X \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.cuda()\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _,dU \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_dU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m getResidue(X, dU)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[276], line 47\u001b[0m, in \u001b[0;36mModel.get_U_dU\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_U)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_U\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_U_harmonic(X_t_))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[276], line 16\u001b[0m, in \u001b[0;36mFCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(X)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 6x10)"
     ]
    }
   ],
   "source": [
    "from Loss import getResidue\n",
    "\n",
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()\n",
    "#def get_a(X,k=choose_id):\n",
    "#    if choose_id==1: return 2*torch.exp(-3*X**2)\n",
    "#    if choose_id==2: return 2/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "#    if choose_id==3: return 4/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "def get_loss(model,data):\n",
    "    X = data\n",
    "    X = torch.tensor(X,requires_grad=True)#.cuda()\n",
    "    _,dU = model.get_U_dU(X)\n",
    "\n",
    "    loss = getResidue(X, dU)\n",
    "    return loss#,loss\n",
    "\n",
    "# plot_model(model)\n",
    "get_loss(model,X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65d4b29e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.type of FCNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=10, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=10, out_features=1, bias=True)\n",
      ")>\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0.]) Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1.]) Parameter containing:\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2831cec7a8e744599451e14ba083edcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jf/w5m4c3sj361fdh8b5wz1cy1m0000gn/T/ipykernel_89714/972852114.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_t_ = torch.tensor(X_t, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.001\u001b[39m))\n\u001b[1;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m _loss_step \u001b[38;5;241m=\u001b[39m \u001b[43mSOL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5e4\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_show_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     15\u001b[0m plot_model(model)\n",
      "Cell \u001b[0;32mIn[78], line 71\u001b[0m, in \u001b[0;36mSolver.train_model\u001b[0;34m(self, data_train, data_test, get_loss, optimizer, n_steps, batch_size, scheduler, n_show_loss, error_model, use_tqdm)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step \u001b[38;5;129;01min\u001b[39;00m step_range:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i_step\u001b[38;5;241m%\u001b[39mn_show_loss\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m         loss_train,loss_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\\\n\u001b[1;32m     72\u001b[0m                                get_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,data_test)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_num\u001b[39m(x): \n\u001b[1;32m     75\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(x)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(x)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m.01\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%0.5f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mx\n",
      "Cell \u001b[0;32mIn[83], line 22\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m X \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.cuda()\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _,dU \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_dU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m getResidue(X, dU)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[78], line 43\u001b[0m, in \u001b[0;36mModel.get_U_dU\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m X_t_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_t, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# X_t_ = (X_t - self.mu) / self.sigma #normalization necessary, column wise is standard\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m U_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_U \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_U\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_U_harmonic(X_t_))) \u001b[38;5;66;03m#*certify validity\u001b[39;00m\n\u001b[1;32m     44\u001b[0m U_list\u001b[38;5;241m.\u001b[39mappend(U_t)\n\u001b[1;32m     45\u001b[0m dU_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     46\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mU_t,\n\u001b[1;32m     47\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mX_t,\n\u001b[1;32m     48\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(U_t),\n\u001b[1;32m     49\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[78], line 16\u001b[0m, in \u001b[0;36mFCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(X)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and Double"
     ]
    }
   ],
   "source": [
    "#for choose_id in [1,2,3]:\n",
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)\n",
    "\n",
    "print(model.mu,model.sigma,model.coef_U)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001))\n",
    "scheduler = None\n",
    "_loss_step = SOL.train_model(data_train=X,data_test=X,\n",
    "                             get_loss=get_loss,optimizer=optimizer,scheduler=scheduler,\n",
    "                             n_steps=int(5e4+1),batch_size=500,n_show_loss=1000,use_tqdm=True)\n",
    "torch.cuda.empty_cache()\n",
    "plot_model(model)\n",
    "   # torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\n",
    "#torch.save(model.state_dict(),\"savee/model_anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77f2b8b6-9f6d-4fae-ae29-87c4a6cb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"savee/model_99\")\n",
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b07f7a-fb98-4405-8790-18bc4afbc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "U_NN     = model.get_U_np(xx)\n",
    "U_NN_min = U_NN.min()\n",
    "U_NN     = U_NN-U_NN_min\n",
    "c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlabel('$x$',fontsize=10)\n",
    "ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "ax.set_xlim([0,2])\n",
    "ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "ax.set_xticks([0,.5,1.,1.5,2])\n",
    "ax.yaxis.grid(linestyle='--')\n",
    "ax.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4d82-80a8-422e-a7d9-d2623106808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf96b220-f226-49f4-8ec6-244b5f5ca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "from scipy.io import savemat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839674fe",
   "metadata": {},
   "source": [
    "# Visualizing the results for different a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6cc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_models(models):\n",
    "    \n",
    "    xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "    \n",
    "    for k,model_name in enumerate(models):\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        U_NN     = model.get_U_np(xx)\n",
    "        U_NN_min = U_NN.min()\n",
    "        U_NN     = U_NN-U_NN_min\n",
    "        c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5,label=\"$a_{%d}(x)$\"%(k+1))\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlabel('$x$',fontsize=10)\n",
    "    ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "    ax.set_xlim([0,2])\n",
    "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "    ax.set_xticks([0,.5,1.,1.5,2])\n",
    "    ax.yaxis.grid(linestyle='--')\n",
    "    ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "plot_models([\"savee/model_anaconda3\"])\n",
    "#plot_models([\"savee/model_a1_update\", \"savee/model_a2_update\", \"savee/model_a3_update\", \"savee/model_a4\", \"savee/model_a5_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c6ed-2669-4f78-a26d-a70b834146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"/Users/annacoletti/Desktop/savee/model_a3_update.mat\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "319e3d20-3380-447c-9b87-866c7b207395",
   "metadata": {},
   "source": [
    "scio.savemat('W2_learned_DL', U_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c8483-83f4-4254-ad76-c0e322cc5499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
