{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d34f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f3121e-b0e4-46b0-ab2a-09540cec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b280ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def EXIT_NOTEBOOK(): os._exit(00)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ba82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "\n",
    "# # ps\n",
    "# import pysindy as ps\n",
    "\n",
    "# sns.set_theme()\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a3b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 6\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339b9d",
   "metadata": {},
   "source": [
    "# Dataset for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 6)\n",
      "[[-2.70285974e-03  1.39602878e-02  5.62987849e-04 -1.68292943e-02\n",
      "   1.22967343e-02 -1.92464173e-03]\n",
      " [-2.10472269e-02 -8.58090878e-05 -2.26724397e-02 -1.14619799e-02\n",
      "  -1.94287637e-03  8.79127191e-03]\n",
      " [-4.03344147e-02  2.35316491e-02 -3.55978907e-02 -3.06230798e-02\n",
      "  -1.42206457e-02  4.49145418e-03]\n",
      " [-5.31550833e-02  3.79125953e-02 -3.55320482e-02 -3.99425964e-02\n",
      "  -2.59510597e-02 -1.49195612e-04]\n",
      " [-4.08986861e-02  5.09946742e-02 -6.01516821e-02 -2.39974592e-02\n",
      "  -4.09694277e-02  6.49373764e-03]\n",
      " [-4.66618825e-02  5.94775324e-02 -8.36973837e-02  8.98094635e-05\n",
      "  -5.27713025e-02  2.83733344e-03]\n",
      " [-3.45511304e-02  6.61730702e-02 -8.74405853e-02 -9.22887975e-03\n",
      "  -5.49388924e-02  1.69516524e-03]\n",
      " [-5.77025503e-02  6.41024089e-02 -6.23134476e-02 -1.52332440e-02\n",
      "  -7.23044537e-02  1.51338863e-02]\n",
      " [-5.95273395e-02  6.60421015e-02 -6.33997324e-02 -2.65733850e-02\n",
      "  -5.89649145e-02  1.86330244e-02]\n",
      " [-9.26439860e-02  7.05791033e-02 -6.68227789e-02 -7.00186963e-03\n",
      "  -2.14525897e-02 -1.93400512e-02]\n",
      " [-9.26439860e-02  7.05791033e-02 -6.68227789e-02 -7.00186963e-03\n",
      "  -2.14525897e-02 -1.93400512e-02]]\n"
     ]
    }
   ],
   "source": [
    "from BeadModel import Simulate\n",
    "from SimulationParameters import *\n",
    "\n",
    "X = Simulation(*Simulate(numSims)).positions[:,:,:,0].reshape(-1,11).T\n",
    "# Z = X.view\n",
    "\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbd757",
   "metadata": {},
   "source": [
    "# Set the NN model and Solver with training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606412e9",
   "metadata": {
    "code_folding": [
     2,
     18,
     19,
     28,
     42,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def relu2(X): return func.relu(X)**2\n",
    "def tanh(X): return func.tanh(X)\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=6,num_hidden=2,hidden_dim=10,act=func.tanh,transform=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers  = nn.ModuleList([nn.Linear(input_dim,hidden_dim)])\n",
    "        for _ in range(num_hidden-1): self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.act     = act\n",
    "        self.out     = nn.Linear(hidden_dim,output_dim)\n",
    "        self.transform = transform\n",
    "    def forward(self,X):\n",
    "        if self.transform is not None: X = self.transform(X)\n",
    "        for layer in self.layers: X = self.act(layer(X))\n",
    "        Y = self.out(X)\n",
    "        return Y\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,dim,model_U,unit_len=int(5e3)):\n",
    "        super().__init__()\n",
    "        self.dim      = dim\n",
    "        self.model_U  = model_U\n",
    "        self.unit_len = unit_len\n",
    "        self.mu       = nn.Parameter(torch.tensor([0.]*dim),requires_grad=False) \n",
    "        self.sigma    = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #above two lines should work, but if something doesn't work, check here! Does current self.mu and self.sigma code work if dim>1. Should return a vector since what we're trying to do is calculate mu\n",
    "        #and sigma separately for each imnputted feature. mu is parameter[0] for each row.\n",
    "        self.coef_U   = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #self.mu       = nn.Parameter(torch.tensor([0.]*dim).cuda(),requires_grad=False)\n",
    "        #self.sigma    = nn.Parameter(torch.tensor([1.]*dim).cuda(),requires_grad=False)\n",
    "        #self.coef_U   = nn.Parameter(torch.tensor(1.).cuda(),requires_grad=False)\n",
    "    def get_U_harmonic(self,X): return torch.sum(X**2,axis=-1)\n",
    "        \n",
    "    \n",
    "    def get_U_dU(self,X):\n",
    "        # normalize and ensure x is a tensor\n",
    "        if not torch.is_tensor(X): X = torch.tensor(X, requires_grad=True)\n",
    "        U = self.model_U(X).view(-1)\n",
    "        dU = torch.autograd.grad(U, X, torch.ones_like(U), create_graph=True)[0]\n",
    "        # dU = dU.T\n",
    "        return U,dU\n",
    "\n",
    "    \n",
    "    def get_U_np(self,X): \n",
    "        U,_ = self.get_U_dU(X);\n",
    "        return U.cpu().data.numpy()\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "    def train_model(self,data_train,data_test,get_loss,optimizer,\n",
    "                    n_steps,batch_size,scheduler=None,n_show_loss=100,error_model=None,use_tqdm=True):\n",
    "        if use_tqdm: step_range = tqdm(range(n_steps))\n",
    "        else: step_range = range(n_steps)\n",
    "        loss_step = []\n",
    "        for i_step in step_range:\n",
    "            if i_step%n_show_loss==0:\n",
    "                loss_train,loss_test = get_loss(self.model,data_train)[:-1],\\\n",
    "                                       get_loss(self.model,data_test)[:-1]\n",
    "                \n",
    "                def show_num(x): \n",
    "                    if abs(x)<100 and abs(x)>.01: return '%0.5f'%x\n",
    "                    else: return '%0.2e'%x\n",
    "                item1 = '%2dk'%np.int_(i_step/1000)\n",
    "                item2 = 'Loss: '+' '.join([show_num(k) for k in loss_train])\n",
    "                item3 = ' '.join([show_num(k) for k in loss_test])\n",
    "                item4 = ''\n",
    "                if error_model is not None: item4 = 'E(QP): %0.4f' % (error_model(self.model))\n",
    "                print(', '.join([item1,item2,item3,item4]))\n",
    "                loss_step = loss_step + [i_step] + [k.cpu().data.numpy() for k in loss_train]\\\n",
    "                                                 + [k.cpu().data.numpy() for k in loss_train]\n",
    "            data_batch = data_train[random.sample(range(len(data_train)),\n",
    "                                                  min(batch_size,len(data_train)))]\n",
    "#             print(i_step,data_batch.shape)\n",
    "            loss = get_loss(self.model,data_batch)[-1]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        if error_model is not None: \n",
    "            print(\"Error: %0.5f\" % (error_model(self.model)))\n",
    "        return loss_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5786a635",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_U_dU(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e8e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.mu.shape)\n",
    "# print(model.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82dfe6",
   "metadata": {},
   "source": [
    "# Set the loss function and Train the model for differen a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c4eeaf",
   "metadata": {
    "code_folding": [
     0,
     12,
     16
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0257, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0248, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0258, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0217, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0249, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0240, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0220, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0220, grad_fn=<LinalgDetBackward0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Loss import getResidue, getAllResidues\n",
    "\n",
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#def get_a(X,k=choose_id):\n",
    "#    if choose_id==1: return 2*torch.exp(-3*X**2)\n",
    "#    if choose_id==2: return 2/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "#    if choose_id==3: return 4/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "\n",
    "def get_loss(model,data):\n",
    "    X = data\n",
    "    X = torch.tensor(X).clone().detach().requires_grad_(True)\n",
    "    _,dU = model.get_U_dU(X)\n",
    "    loss = getAllResidues(X, dU)\n",
    "    return loss#,loss\n",
    "\n",
    "\n",
    "# plot_model(model)\n",
    "get_loss(model,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d4b29e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe2aa45c1934e95b6d85a875a587b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0k, Loss: -0.04828 -0.04764 -0.05372 -0.05575 -0.05958 -0.05772 -0.06305 -0.05873 -0.06165 -0.04936, -0.04828 -0.04764 -0.05372 -0.05575 -0.05958 -0.05772 -0.06305 -0.05873 -0.06165 -0.04936, \n",
      " 1k, Loss: -0.11973 -0.11893 -0.12501 -0.13138 -0.12963 -0.12728 -0.13154 -0.12806 -0.13402 -0.14809, -0.11973 -0.11893 -0.12501 -0.13138 -0.12963 -0.12728 -0.13154 -0.12806 -0.13402 -0.14809, \n",
      " 2k, Loss: -0.11973 -0.11903 -0.12488 -0.13089 -0.12993 -0.12793 -0.13270 -0.12814 -0.13399 -0.14815, -0.11973 -0.11903 -0.12488 -0.13089 -0.12993 -0.12793 -0.13270 -0.12814 -0.13399 -0.14815, \n",
      " 3k, Loss: -0.11987 -0.11911 -0.12526 -0.13157 -0.13044 -0.12781 -0.13266 -0.12886 -0.13503 -0.14668, -0.11987 -0.11911 -0.12526 -0.13157 -0.13044 -0.12781 -0.13266 -0.12886 -0.13503 -0.14668, \n",
      " 4k, Loss: -0.11974 -0.11909 -0.12523 -0.13142 -0.13052 -0.12803 -0.13305 -0.12896 -0.13506 -0.14703, -0.11974 -0.11909 -0.12523 -0.13142 -0.13052 -0.12803 -0.13305 -0.12896 -0.13506 -0.14703, \n",
      " 5k, Loss: -0.11955 -0.11880 -0.12467 -0.13075 -0.12892 -0.12690 -0.13085 -0.12733 -0.13298 -0.15250, -0.11955 -0.11880 -0.12467 -0.13075 -0.12892 -0.12690 -0.13085 -0.12733 -0.13298 -0.15250, \n",
      " 6k, Loss: -0.11948 -0.11877 -0.12454 -0.13039 -0.12868 -0.12693 -0.13081 -0.12696 -0.13248 -0.15359, -0.11948 -0.11877 -0.12454 -0.13039 -0.12868 -0.12693 -0.13081 -0.12696 -0.13248 -0.15359, \n",
      " 7k, Loss: -0.11989 -0.11933 -0.12533 -0.13135 -0.13135 -0.12858 -0.13440 -0.13000 -0.13608 -0.14511, -0.11989 -0.11933 -0.12533 -0.13135 -0.13135 -0.12858 -0.13440 -0.13000 -0.13608 -0.14511, \n",
      " 8k, Loss: -0.12001 -0.11943 -0.12523 -0.13139 -0.13152 -0.12846 -0.13441 -0.13060 -0.13646 -0.14587, -0.12001 -0.11943 -0.12523 -0.13139 -0.13152 -0.12846 -0.13441 -0.13060 -0.13646 -0.14587, \n",
      " 9k, Loss: -0.11999 -0.11969 -0.12534 -0.13083 -0.13222 -0.12913 -0.13659 -0.13140 -0.13678 -0.15240, -0.11999 -0.11969 -0.12534 -0.13083 -0.13222 -0.12913 -0.13659 -0.13140 -0.13678 -0.15240, \n",
      "10k, Loss: -0.11999 -0.11959 -0.12533 -0.13137 -0.13207 -0.12891 -0.13625 -0.13170 -0.13698 -0.15871, -0.11999 -0.11959 -0.12533 -0.13137 -0.13207 -0.12891 -0.13625 -0.13170 -0.13698 -0.15871, \n",
      "11k, Loss: -0.11945 -0.11817 -0.12510 -0.13082 -0.13203 -0.12897 -0.13683 -0.12997 -0.13656 -0.15878, -0.11945 -0.11817 -0.12510 -0.13082 -0.13203 -0.12897 -0.13683 -0.12997 -0.13656 -0.15878, \n",
      "12k, Loss: -0.11976 -0.11886 -0.12513 -0.13113 -0.13191 -0.12920 -0.13667 -0.13074 -0.13645 -0.15862, -0.11976 -0.11886 -0.12513 -0.13113 -0.13191 -0.12920 -0.13667 -0.13074 -0.13645 -0.15862, \n",
      "13k, Loss: -0.11989 -0.11967 -0.12506 -0.13072 -0.13199 -0.12929 -0.13709 -0.13145 -0.13656 -0.15943, -0.11989 -0.11967 -0.12506 -0.13072 -0.13199 -0.12929 -0.13709 -0.13145 -0.13656 -0.15943, \n",
      "14k, Loss: -0.11970 -0.11978 -0.12491 -0.13075 -0.13173 -0.12874 -0.13639 -0.13177 -0.13660 -0.15932, -0.11970 -0.11978 -0.12491 -0.13075 -0.13173 -0.12874 -0.13639 -0.13177 -0.13660 -0.15932, \n",
      "15k, Loss: -0.11992 -0.11944 -0.12513 -0.13050 -0.13225 -0.12904 -0.13690 -0.13132 -0.13703 -0.15688, -0.11992 -0.11944 -0.12513 -0.13050 -0.13225 -0.12904 -0.13690 -0.13132 -0.13703 -0.15688, \n",
      "16k, Loss: -0.11991 -0.11937 -0.12510 -0.13092 -0.13179 -0.12949 -0.13685 -0.13096 -0.13603 -0.15854, -0.11991 -0.11937 -0.12510 -0.13092 -0.13179 -0.12949 -0.13685 -0.13096 -0.13603 -0.15854, \n",
      "17k, Loss: -0.11992 -0.11943 -0.12515 -0.13086 -0.13208 -0.12921 -0.13697 -0.13149 -0.13673 -0.15961, -0.11992 -0.11943 -0.12515 -0.13086 -0.13208 -0.12921 -0.13697 -0.13149 -0.13673 -0.15961, \n",
      "18k, Loss: -0.11993 -0.11964 -0.12524 -0.13093 -0.13213 -0.12928 -0.13701 -0.13141 -0.13674 -0.15914, -0.11993 -0.11964 -0.12524 -0.13093 -0.13213 -0.12928 -0.13701 -0.13141 -0.13674 -0.15914, \n",
      "19k, Loss: -0.11990 -0.11965 -0.12518 -0.13088 -0.13206 -0.12917 -0.13691 -0.13153 -0.13667 -0.15958, -0.11990 -0.11965 -0.12518 -0.13088 -0.13206 -0.12917 -0.13691 -0.13153 -0.13667 -0.15958, \n",
      "20k, Loss: -0.11986 -0.11973 -0.12515 -0.13084 -0.13203 -0.12891 -0.13664 -0.13168 -0.13678 -0.15962, -0.11986 -0.11973 -0.12515 -0.13084 -0.13203 -0.12891 -0.13664 -0.13168 -0.13678 -0.15962, \n",
      "21k, Loss: -0.11989 -0.11907 -0.12523 -0.13123 -0.13213 -0.12912 -0.13671 -0.13133 -0.13689 -0.15928, -0.11989 -0.11907 -0.12523 -0.13123 -0.13213 -0.12912 -0.13671 -0.13133 -0.13689 -0.15928, \n",
      "22k, Loss: -0.11995 -0.11938 -0.12530 -0.13107 -0.13220 -0.12926 -0.13691 -0.13133 -0.13679 -0.15953, -0.11995 -0.11938 -0.12530 -0.13107 -0.13220 -0.12926 -0.13691 -0.13133 -0.13679 -0.15953, \n",
      "23k, Loss: -0.11994 -0.11936 -0.12528 -0.13099 -0.13224 -0.12925 -0.13700 -0.13126 -0.13686 -0.15961, -0.11994 -0.11936 -0.12528 -0.13099 -0.13224 -0.12925 -0.13700 -0.13126 -0.13686 -0.15961, \n",
      "24k, Loss: -0.11990 -0.11965 -0.12524 -0.13081 -0.13220 -0.12918 -0.13703 -0.13145 -0.13679 -0.15966, -0.11990 -0.11965 -0.12524 -0.13081 -0.13220 -0.12918 -0.13703 -0.13145 -0.13679 -0.15966, \n",
      "25k, Loss: -0.11984 -0.11958 -0.12525 -0.13140 -0.13161 -0.12859 -0.13535 -0.13100 -0.13621 -0.15823, -0.11984 -0.11958 -0.12525 -0.13140 -0.13161 -0.12859 -0.13535 -0.13100 -0.13621 -0.15823, \n",
      "26k, Loss: -0.11966 -0.11984 -0.12517 -0.13081 -0.13200 -0.12867 -0.13643 -0.13156 -0.13679 -0.15961, -0.11966 -0.11984 -0.12517 -0.13081 -0.13200 -0.12867 -0.13643 -0.13156 -0.13679 -0.15961, \n",
      "27k, Loss: -0.11997 -0.11939 -0.12534 -0.13123 -0.13219 -0.12924 -0.13679 -0.13123 -0.13678 -0.15929, -0.11997 -0.11939 -0.12534 -0.13123 -0.13219 -0.12924 -0.13679 -0.13123 -0.13678 -0.15929, \n",
      "28k, Loss: -0.11986 -0.11976 -0.12526 -0.13084 -0.13225 -0.12895 -0.13682 -0.13155 -0.13701 -0.15938, -0.11986 -0.11976 -0.12526 -0.13084 -0.13225 -0.12895 -0.13682 -0.13155 -0.13701 -0.15938, \n",
      "29k, Loss: -0.11981 -0.11978 -0.12532 -0.13105 -0.13204 -0.12901 -0.13660 -0.13131 -0.13651 -0.15916, -0.11981 -0.11978 -0.12532 -0.13105 -0.13204 -0.12901 -0.13660 -0.13131 -0.13651 -0.15916, \n",
      "30k, Loss: -0.11987 -0.11975 -0.12537 -0.13120 -0.13223 -0.12873 -0.13634 -0.13168 -0.13718 -0.15965, -0.11987 -0.11975 -0.12537 -0.13120 -0.13223 -0.12873 -0.13634 -0.13168 -0.13718 -0.15965, \n",
      "31k, Loss: -0.11990 -0.11970 -0.12529 -0.13099 -0.13214 -0.12924 -0.13694 -0.13131 -0.13657 -0.15887, -0.11990 -0.11970 -0.12529 -0.13099 -0.13214 -0.12924 -0.13694 -0.13131 -0.13657 -0.15887, \n",
      "32k, Loss: -0.11994 -0.11961 -0.12532 -0.13115 -0.13226 -0.12913 -0.13686 -0.13139 -0.13694 -0.15957, -0.11994 -0.11961 -0.12532 -0.13115 -0.13226 -0.12913 -0.13686 -0.13139 -0.13694 -0.15957, \n",
      "33k, Loss: -0.11985 -0.11978 -0.12531 -0.13096 -0.13221 -0.12904 -0.13683 -0.13144 -0.13685 -0.15973, -0.11985 -0.11978 -0.12531 -0.13096 -0.13221 -0.12904 -0.13683 -0.13144 -0.13685 -0.15973, \n",
      "34k, Loss: -0.11986 -0.11977 -0.12535 -0.13117 -0.13226 -0.12880 -0.13652 -0.13165 -0.13716 -0.15969, -0.11986 -0.11977 -0.12535 -0.13117 -0.13226 -0.12880 -0.13652 -0.13165 -0.13716 -0.15969, \n",
      "35k, Loss: -0.11986 -0.11975 -0.12526 -0.13096 -0.13225 -0.12889 -0.13672 -0.13163 -0.13708 -0.15975, -0.11986 -0.11975 -0.12526 -0.13096 -0.13225 -0.12889 -0.13672 -0.13163 -0.13708 -0.15975, \n",
      "36k, Loss: -0.11986 -0.11978 -0.12544 -0.13136 -0.13226 -0.12873 -0.13628 -0.13163 -0.13718 -0.15964, -0.11986 -0.11978 -0.12544 -0.13136 -0.13226 -0.12873 -0.13628 -0.13163 -0.13718 -0.15964, \n",
      "37k, Loss: -0.11997 -0.11932 -0.12531 -0.13125 -0.13220 -0.12911 -0.13661 -0.13118 -0.13687 -0.15970, -0.11997 -0.11932 -0.12531 -0.13125 -0.13220 -0.12911 -0.13661 -0.13118 -0.13687 -0.15970, \n",
      "38k, Loss: -0.11990 -0.11912 -0.12513 -0.13115 -0.13208 -0.12914 -0.13666 -0.13108 -0.13672 -0.15936, -0.11990 -0.11912 -0.12513 -0.13115 -0.13208 -0.12914 -0.13666 -0.13108 -0.13672 -0.15936, \n",
      "39k, Loss: -0.11987 -0.11979 -0.12534 -0.13091 -0.13226 -0.12905 -0.13683 -0.13143 -0.13686 -0.15974, -0.11987 -0.11979 -0.12534 -0.13091 -0.13226 -0.12905 -0.13683 -0.13143 -0.13686 -0.15974, \n",
      "40k, Loss: -0.11982 -0.11980 -0.12533 -0.13118 -0.13222 -0.12844 -0.13612 -0.13184 -0.13734 -0.15971, -0.11982 -0.11980 -0.12533 -0.13118 -0.13222 -0.12844 -0.13612 -0.13184 -0.13734 -0.15971, \n",
      "41k, Loss: -0.11998 -0.11968 -0.12540 -0.13158 -0.13215 -0.12889 -0.13613 -0.13154 -0.13695 -0.15925, -0.11998 -0.11968 -0.12540 -0.13158 -0.13215 -0.12889 -0.13613 -0.13154 -0.13695 -0.15925, \n",
      "42k, Loss: -0.11989 -0.11979 -0.12533 -0.13100 -0.13229 -0.12899 -0.13675 -0.13151 -0.13699 -0.15942, -0.11989 -0.11979 -0.12533 -0.13100 -0.13229 -0.12899 -0.13675 -0.13151 -0.13699 -0.15942, \n",
      "43k, Loss: -0.12003 -0.11943 -0.12532 -0.13166 -0.13203 -0.12886 -0.13590 -0.13125 -0.13688 -0.15826, -0.12003 -0.11943 -0.12532 -0.13166 -0.13203 -0.12886 -0.13590 -0.13125 -0.13688 -0.15826, \n",
      "44k, Loss: -0.11996 -0.11963 -0.12534 -0.13113 -0.13227 -0.12909 -0.13674 -0.13139 -0.13700 -0.15977, -0.11996 -0.11963 -0.12534 -0.13113 -0.13227 -0.12909 -0.13674 -0.13139 -0.13700 -0.15977, \n",
      "45k, Loss: -0.11990 -0.11976 -0.12526 -0.13091 -0.13227 -0.12910 -0.13700 -0.13145 -0.13696 -0.15979, -0.11990 -0.11976 -0.12526 -0.13091 -0.13227 -0.12910 -0.13700 -0.13145 -0.13696 -0.15979, \n",
      "46k, Loss: -0.11994 -0.11977 -0.12540 -0.13132 -0.13227 -0.12897 -0.13662 -0.13153 -0.13705 -0.15970, -0.11994 -0.11977 -0.12540 -0.13132 -0.13227 -0.12897 -0.13662 -0.13153 -0.13705 -0.15970, \n",
      "47k, Loss: -0.11993 -0.11975 -0.12528 -0.13084 -0.13224 -0.12937 -0.13725 -0.13122 -0.13665 -0.15979, -0.11993 -0.11975 -0.12528 -0.13084 -0.13224 -0.12937 -0.13725 -0.13122 -0.13665 -0.15979, \n",
      "48k, Loss: -0.11993 -0.11984 -0.12542 -0.13125 -0.13230 -0.12880 -0.13658 -0.13158 -0.13712 -0.15975, -0.11993 -0.11984 -0.12542 -0.13125 -0.13230 -0.12880 -0.13658 -0.13158 -0.13712 -0.15975, \n",
      "49k, Loss: -0.11999 -0.11979 -0.12548 -0.13143 -0.13228 -0.12895 -0.13657 -0.13149 -0.13711 -0.15954, -0.11999 -0.11979 -0.12548 -0.13143 -0.13228 -0.12895 -0.13657 -0.13149 -0.13711 -0.15954, \n",
      "50k, Loss: -0.11994 -0.11982 -0.12533 -0.13107 -0.13224 -0.12922 -0.13710 -0.13137 -0.13682 -0.15966, -0.11994 -0.11982 -0.12533 -0.13107 -0.13224 -0.12922 -0.13710 -0.13137 -0.13682 -0.15966, \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x1 and 6x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m _loss_step \u001b[38;5;241m=\u001b[39m SOL\u001b[38;5;241m.\u001b[39mtrain_model(data_train\u001b[38;5;241m=\u001b[39mX,data_test\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m     11\u001b[0m                              get_loss\u001b[38;5;241m=\u001b[39mget_loss,optimizer\u001b[38;5;241m=\u001b[39moptimizer,scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m     12\u001b[0m                              n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5e4\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,n_show_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,use_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(),\"savee/model_anaconda3\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, cmap, max_V)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_model\u001b[39m(model,cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrain\u001b[39m\u001b[38;5;124m'\u001b[39m,max_V \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     xx     \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     U_NN   \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     U_NN_min \u001b[38;5;241m=\u001b[39m U_NN\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m      8\u001b[0m     U_NN  \u001b[38;5;241m=\u001b[39m U_NN\u001b[38;5;241m-\u001b[39mU_NN_min\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mModel.get_U_np\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_U_np\u001b[39m(\u001b[38;5;28mself\u001b[39m,X): \n\u001b[0;32m---> 45\u001b[0m     U,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_dU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mModel.get_U_dU\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_U_dU\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# normalize and ensure x is a tensor\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(X): X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m     U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_U\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     dU \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(U, X, torch\u001b[38;5;241m.\u001b[39mones_like(U), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# dU = dU.T\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mFCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(X)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x1 and 6x10)"
     ]
    }
   ],
   "source": [
    "#for choose_id in [1,2,3]:\n",
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)\n",
    "\n",
    "# print(model.mu,model.sigma,model.coef_U)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001))\n",
    "scheduler = None\n",
    "_loss_step = SOL.train_model(data_train=X,data_test=X,\n",
    "                             get_loss=get_loss,optimizer=optimizer,scheduler=scheduler,\n",
    "                             n_steps=int(5e4+1),batch_size=500,n_show_loss=1000,use_tqdm=True)\n",
    "torch.cuda.empty_cache()\n",
    "plot_model(model)\n",
    "# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\n",
    "#torch.save(model.state_dict(),\"savee/model_anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77f2b8b6-9f6d-4fae-ae29-87c4a6cb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"savee/model_99\")\n",
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b07f7a-fb98-4405-8790-18bc4afbc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "# xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "# U_NN     = model.get_U_np(xx)\n",
    "# U_NN_min = U_NN.min()\n",
    "# U_NN     = U_NN-U_NN_min\n",
    "# c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "# ax.legend(fontsize=10)\n",
    "# ax.set_xlabel('$x$',fontsize=10)\n",
    "# ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "# ax.set_xlim([0,2])\n",
    "# ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "# ax.set_xticks([0,.5,1.,1.5,2])\n",
    "# ax.yaxis.grid(linestyle='--')\n",
    "# ax.tick_params(axis=\"both\", labelsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4d82-80a8-422e-a7d9-d2623106808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839674fe",
   "metadata": {},
   "source": [
    "# Visualizing the results for different a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6cc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_models(models):\n",
    "    \n",
    "    xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "    \n",
    "    for k,model_name in enumerate(models):\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        U_NN     = model.get_U_np(xx)\n",
    "        U_NN_min = U_NN.min()\n",
    "        U_NN     = U_NN-U_NN_min\n",
    "        c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5,label=\"$a_{%d}(x)$\"%(k+1))\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlabel('$x$',fontsize=10)\n",
    "    ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "    ax.set_xlim([0,2])\n",
    "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "    ax.set_xticks([0,.5,1.,1.5,2])\n",
    "    ax.yaxis.grid(linestyle='--')\n",
    "    ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_models([\"savee/model_anaconda3\"])\n",
    "#plot_models([\"savee/model_a1_update\", \"savee/model_a2_update\", \"savee/model_a3_update\", \"savee/model_a4\", \"savee/model_a5_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c6ed-2669-4f78-a26d-a70b834146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"/Users/annacoletti/Desktop/savee/model_a3_update.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e3d20-3380-447c-9b87-866c7b207395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as scio\n",
    "# from scipy.io import savemat\n",
    "# scio.savemat('W2_learned_DL', U_NN)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
