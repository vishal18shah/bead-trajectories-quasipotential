{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d34f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f3121e-b0e4-46b0-ab2a-09540cec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b280ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def EXIT_NOTEBOOK(): os._exit(00)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ba82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "\n",
    "# # ps\n",
    "# import pysindy as ps\n",
    "\n",
    "# sns.set_theme()\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a3b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 6\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339b9d",
   "metadata": {},
   "source": [
    "# Dataset for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "818019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 6)\n",
      "[[ 0.03581867 -0.01326625 -0.01137841  0.03130418 -0.00570436 -0.02530064]\n",
      " [ 0.00846129 -0.01429928 -0.02668912  0.01348121  0.01245368 -0.04547064]\n",
      " [ 0.03349367 -0.03546511  0.00020097 -0.00405384  0.00846287 -0.04162075]\n",
      " [ 0.04016371 -0.04121816 -0.01788788 -0.0010874   0.01548763 -0.01288343]\n",
      " [ 0.07802293 -0.04863203 -0.04769198 -0.00876889  0.02132241 -0.01528442]\n",
      " [ 0.06694652 -0.0702052  -0.0614259  -0.02902381 -0.00988916 -0.00749221]\n",
      " [ 0.08704529 -0.10143258 -0.06550138 -0.03027692 -0.01473048  0.01824377]\n",
      " [ 0.05423869 -0.11830069 -0.03851321 -0.03761877 -0.02997092  0.01582603]\n",
      " [ 0.02084926 -0.10456833 -0.0495629  -0.03032988 -0.03070538  0.01314817]\n",
      " [ 0.03717264 -0.11486016 -0.03687622 -0.0313549  -0.02749083  0.00014137]\n",
      " [ 0.03717264 -0.11486016 -0.03687622 -0.0313549  -0.02749083  0.00014137]]\n"
     ]
    }
   ],
   "source": [
    "from BeadModel import Simulate\n",
    "from SimulationParameters import *\n",
    "\n",
    "X = Simulation(*Simulate(numSims)).positions[:,:,:,0].reshape(-1,11).T\n",
    "# Z = X.view\n",
    "\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbd757",
   "metadata": {},
   "source": [
    "# Set the NN model and Solver with training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "606412e9",
   "metadata": {
    "code_folding": [
     2,
     18,
     19,
     28,
     42,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def relu2(X): return func.relu(X)**2\n",
    "def tanh(X): return func.tanh(X)\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=6,num_hidden=2,hidden_dim=10,act=func.tanh,transform=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers  = nn.ModuleList([nn.Linear(input_dim,hidden_dim)])\n",
    "        for _ in range(num_hidden-1): self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.act     = act\n",
    "        self.out     = nn.Linear(hidden_dim,output_dim)\n",
    "        self.transform = transform\n",
    "    def forward(self,X):\n",
    "        if self.transform is not None: X = self.transform(X)\n",
    "        for layer in self.layers: X = self.act(layer(X))\n",
    "        Y = self.out(X)\n",
    "        return Y\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,dim,model_U,unit_len=int(5e3)):\n",
    "        super().__init__()\n",
    "        self.dim      = dim\n",
    "        self.model_U  = model_U\n",
    "        self.unit_len = unit_len\n",
    "        self.mu       = nn.Parameter(torch.tensor([0.]*dim),requires_grad=False) \n",
    "        self.sigma    = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #above two lines should work, but if something doesn't work, check here! Does current self.mu and self.sigma code work if dim>1. Should return a vector since what we're trying to do is calculate mu\n",
    "        #and sigma separately for each imnputted feature. mu is parameter[0] for each row.\n",
    "        self.coef_U   = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #self.mu       = nn.Parameter(torch.tensor([0.]*dim).cuda(),requires_grad=False)\n",
    "        #self.sigma    = nn.Parameter(torch.tensor([1.]*dim).cuda(),requires_grad=False)\n",
    "        #self.coef_U   = nn.Parameter(torch.tensor(1.).cuda(),requires_grad=False)\n",
    "    def get_U_harmonic(self,X): return torch.sum(X**2,axis=-1)\n",
    "        \n",
    "    \n",
    "    def get_U_dU(self,X):\n",
    "        # normalize and ensure x is a tensor\n",
    "        if not torch.is_tensor(X): X = torch.tensor(X, requires_grad=True)\n",
    "        U = self.model_U(X).view(-1)\n",
    "        dU = torch.autograd.grad(U, X, torch.ones_like(U), create_graph=True)[0]\n",
    "        # dU = dU.T\n",
    "        return U,dU\n",
    "\n",
    "    \n",
    "    def get_U_np(self,X): \n",
    "        U,_ = self.get_U_dU(X);\n",
    "        return U.cpu().data.numpy()\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "    def train_model(self,data_train,data_test,get_loss,optimizer,\n",
    "                    n_steps,batch_size,scheduler=None,n_show_loss=100,error_model=None,use_tqdm=True):\n",
    "        if use_tqdm: step_range = tqdm(range(n_steps))\n",
    "        else: step_range = range(n_steps)\n",
    "        loss_step = []\n",
    "        for i_step in step_range:\n",
    "            if i_step%n_show_loss==0:\n",
    "                loss_train,loss_test = get_loss(self.model,data_train)[:-1],\\\n",
    "                                       get_loss(self.model,data_test)[:-1]\n",
    "                \n",
    "                def show_num(x): \n",
    "                    if abs(x)<100 and abs(x)>.01: return '%0.5f'%x\n",
    "                    else: return '%0.2e'%x\n",
    "                item1 = '%2dk'%np.int_(i_step/1000)\n",
    "                item2 = 'Loss: '+' '.join([show_num(k) for k in loss_train])\n",
    "                item3 = ' '.join([show_num(k) for k in loss_test])\n",
    "                item4 = ''\n",
    "                if error_model is not None: item4 = 'E(QP): %0.4f' % (error_model(self.model))\n",
    "                print(', '.join([item1,item2,item3,item4]))\n",
    "                loss_step = loss_step + [i_step] + [k.cpu().data.numpy() for k in loss_train]\\\n",
    "                                                 + [k.cpu().data.numpy() for k in loss_train]\n",
    "            data_batch = data_train[random.sample(range(len(data_train)),\n",
    "                                                  min(batch_size,len(data_train)))]\n",
    "#             print(i_step,data_batch.shape)\n",
    "            loss = get_loss(self.model,data_batch)[-1]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        if error_model is not None: \n",
    "            print(\"Error: %0.5f\" % (error_model(self.model)))\n",
    "        return loss_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5786a635",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd4f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_U_dU(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94e8e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.mu.shape)\n",
    "# print(model.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5df5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82dfe6",
   "metadata": {},
   "source": [
    "# Set the loss function and Train the model for differen a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69c4eeaf",
   "metadata": {
    "code_folding": [
     0,
     12,
     16
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0531, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0429, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0504, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0542, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0605, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0718, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0694, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0630, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0654, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0654, grad_fn=<LinalgDetBackward0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Loss import getResidue, getAllResidues\n",
    "\n",
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#def get_a(X,k=choose_id):\n",
    "#    if choose_id==1: return 2*torch.exp(-3*X**2)\n",
    "#    if choose_id==2: return 2/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "#    if choose_id==3: return 4/(1+torch.exp(20*(torch.abs(X)-0.75)))\n",
    "\n",
    "def get_loss(model,data):\n",
    "    X = data\n",
    "    X = torch.tensor(X).clone().detach().requires_grad_(True)\n",
    "    _,dU = model.get_U_dU(X)\n",
    "    loss = getAllResidues(X, dU)\n",
    "    return loss#,loss\n",
    "\n",
    "\n",
    "# plot_model(model)\n",
    "get_loss(model,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65d4b29e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb5e9ade28e48b6b8ccaf1469c300b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0k, Loss: -0.02347 -0.02745 -0.02588 -0.02508 -0.02507 -0.02591 -0.02683 -0.02581 -0.02504 -0.02358, -0.02347 -0.02745 -0.02588 -0.02508 -0.02507 -0.02591 -0.02683 -0.02581 -0.02504 -0.02358, \n",
      " 1k, Loss: -0.11801 -0.11986 -0.11631 -0.12491 -0.15682 -0.17750 -0.25627 -0.19751 -0.16555 -0.17111, -0.11801 -0.11986 -0.11631 -0.12491 -0.15682 -0.17750 -0.25627 -0.19751 -0.16555 -0.17111, \n",
      " 2k, Loss: -0.11716 -0.11910 -0.11513 -0.12361 -0.15475 -0.17570 -0.25709 -0.19887 -0.16613 -0.17233, -0.11716 -0.11910 -0.11513 -0.12361 -0.15475 -0.17570 -0.25709 -0.19887 -0.16613 -0.17233, \n",
      " 3k, Loss: -0.11787 -0.11993 -0.11657 -0.12506 -0.15730 -0.17806 -0.25708 -0.19723 -0.16544 -0.17060, -0.11787 -0.11993 -0.11657 -0.12506 -0.15730 -0.17806 -0.25708 -0.19723 -0.16544 -0.17060, \n",
      " 4k, Loss: -0.11604 -0.11801 -0.11427 -0.12267 -0.15525 -0.17699 -0.26048 -0.19790 -0.16467 -0.17059, -0.11604 -0.11801 -0.11427 -0.12267 -0.15525 -0.17699 -0.26048 -0.19790 -0.16467 -0.17059, \n",
      " 5k, Loss: -0.11766 -0.12003 -0.11614 -0.12422 -0.15392 -0.17534 -0.25698 -0.20034 -0.16706 -0.17309, -0.11766 -0.12003 -0.11614 -0.12422 -0.15392 -0.17534 -0.25698 -0.20034 -0.16706 -0.17309, \n",
      " 6k, Loss: -0.11735 -0.11976 -0.11607 -0.12407 -0.15485 -0.17700 -0.25977 -0.19981 -0.16596 -0.17163, -0.11735 -0.11976 -0.11607 -0.12407 -0.15485 -0.17700 -0.25977 -0.19981 -0.16596 -0.17163, \n",
      " 7k, Loss: -0.11778 -0.12055 -0.11657 -0.12432 -0.15245 -0.17394 -0.25650 -0.20143 -0.16799 -0.17395, -0.11778 -0.12055 -0.11657 -0.12432 -0.15245 -0.17394 -0.25650 -0.20143 -0.16799 -0.17395, \n",
      " 8k, Loss: -0.11823 -0.12091 -0.11706 -0.12488 -0.15323 -0.17424 -0.25661 -0.20122 -0.16830 -0.17418, -0.11823 -0.12091 -0.11706 -0.12488 -0.15323 -0.17424 -0.25661 -0.20122 -0.16830 -0.17418, \n",
      " 9k, Loss: -0.11838 -0.12019 -0.11703 -0.12553 -0.15962 -0.17914 -0.26083 -0.19597 -0.16540 -0.17092, -0.11838 -0.12019 -0.11703 -0.12553 -0.15962 -0.17914 -0.26083 -0.19597 -0.16540 -0.17092, \n",
      "10k, Loss: -0.11844 -0.12022 -0.11710 -0.12544 -0.15993 -0.18001 -0.26284 -0.19626 -0.16524 -0.17079, -0.11844 -0.12022 -0.11710 -0.12544 -0.15993 -0.18001 -0.26284 -0.19626 -0.16524 -0.17079, \n",
      "11k, Loss: -0.11809 -0.12105 -0.11704 -0.12461 -0.15245 -0.17389 -0.25872 -0.20362 -0.16983 -0.17546, -0.11809 -0.12105 -0.11704 -0.12461 -0.15245 -0.17389 -0.25872 -0.20362 -0.16983 -0.17546, \n",
      "12k, Loss: -0.11888 -0.12157 -0.11742 -0.12514 -0.16272 -0.18114 -0.26276 -0.19826 -0.16821 -0.17511, -0.11888 -0.12157 -0.11742 -0.12514 -0.16272 -0.18114 -0.26276 -0.19826 -0.16821 -0.17511, \n",
      "13k, Loss: -0.11948 -0.12250 -0.11762 -0.12464 -0.16165 -0.18121 -0.26415 -0.20041 -0.16817 -0.17582, -0.11948 -0.12250 -0.11762 -0.12464 -0.16165 -0.18121 -0.26415 -0.20041 -0.16817 -0.17582, \n",
      "14k, Loss: -0.11857 -0.12142 -0.11738 -0.12510 -0.16313 -0.18290 -0.26067 -0.20078 -0.16926 -0.17589, -0.11857 -0.12142 -0.11738 -0.12510 -0.16313 -0.18290 -0.26067 -0.20078 -0.16926 -0.17589, \n",
      "15k, Loss: -0.11931 -0.12285 -0.11764 -0.12435 -0.15911 -0.18002 -0.26370 -0.20340 -0.16953 -0.17673, -0.11931 -0.12285 -0.11764 -0.12435 -0.15911 -0.18002 -0.26370 -0.20340 -0.16953 -0.17673, \n",
      "16k, Loss: -0.11920 -0.12211 -0.11763 -0.12504 -0.16432 -0.18330 -0.26292 -0.20079 -0.16932 -0.17643, -0.11920 -0.12211 -0.11763 -0.12504 -0.16432 -0.18330 -0.26292 -0.20079 -0.16932 -0.17643, \n",
      "17k, Loss: -0.11893 -0.12238 -0.11758 -0.12466 -0.16259 -0.18246 -0.26222 -0.20301 -0.17041 -0.17705, -0.11893 -0.12238 -0.11758 -0.12466 -0.16259 -0.18246 -0.26222 -0.20301 -0.17041 -0.17705, \n",
      "18k, Loss: -0.11924 -0.12162 -0.11762 -0.12540 -0.16719 -0.18422 -0.25751 -0.19964 -0.16917 -0.17602, -0.11924 -0.12162 -0.11762 -0.12540 -0.16719 -0.18422 -0.25751 -0.19964 -0.16917 -0.17602, \n",
      "19k, Loss: -0.12001 -0.12200 -0.11793 -0.12510 -0.16517 -0.18299 -0.26357 -0.20257 -0.16946 -0.17672, -0.12001 -0.12200 -0.11793 -0.12510 -0.16517 -0.18299 -0.26357 -0.20257 -0.16946 -0.17672, \n",
      "20k, Loss: -0.12167 -0.12157 -0.11849 -0.12544 -0.16761 -0.18194 -0.26230 -0.20373 -0.16933 -0.17660, -0.12167 -0.12157 -0.11849 -0.12544 -0.16761 -0.18194 -0.26230 -0.20373 -0.16933 -0.17660, \n",
      "21k, Loss: -0.12432 -0.12120 -0.11926 -0.12581 -0.17126 -0.18247 -0.26395 -0.20196 -0.16799 -0.17462, -0.12432 -0.12120 -0.11926 -0.12581 -0.17126 -0.18247 -0.26395 -0.20196 -0.16799 -0.17462, \n",
      "22k, Loss: -0.12468 -0.12235 -0.11955 -0.12523 -0.17160 -0.18326 -0.26167 -0.20370 -0.16993 -0.17657, -0.12468 -0.12235 -0.11955 -0.12523 -0.17160 -0.18326 -0.26167 -0.20370 -0.16993 -0.17657, \n",
      "23k, Loss: -0.12571 -0.12239 -0.11995 -0.12521 -0.17059 -0.18132 -0.26309 -0.20285 -0.16806 -0.17456, -0.12571 -0.12239 -0.11995 -0.12521 -0.17059 -0.18132 -0.26309 -0.20285 -0.16806 -0.17456, \n",
      "24k, Loss: -0.12555 -0.12254 -0.11986 -0.12515 -0.17139 -0.18318 -0.26284 -0.20392 -0.16960 -0.17609, -0.12555 -0.12254 -0.11986 -0.12515 -0.17139 -0.18318 -0.26284 -0.20392 -0.16960 -0.17609, \n",
      "25k, Loss: -0.12507 -0.12203 -0.11966 -0.12575 -0.17225 -0.18430 -0.26207 -0.20360 -0.16968 -0.17636, -0.12507 -0.12203 -0.11966 -0.12575 -0.17225 -0.18430 -0.26207 -0.20360 -0.16968 -0.17636, \n",
      "26k, Loss: -0.12512 -0.12257 -0.11964 -0.12545 -0.17328 -0.18514 -0.25805 -0.20370 -0.17050 -0.17644, -0.12512 -0.12257 -0.11964 -0.12545 -0.17328 -0.18514 -0.25805 -0.20370 -0.17050 -0.17644, \n",
      "27k, Loss: -0.12532 -0.12226 -0.11981 -0.12592 -0.17228 -0.18433 -0.25674 -0.20155 -0.16738 -0.17324, -0.12532 -0.12226 -0.11981 -0.12592 -0.17228 -0.18433 -0.25674 -0.20155 -0.16738 -0.17324, \n",
      "28k, Loss: -0.12515 -0.12270 -0.11960 -0.12516 -0.17300 -0.18489 -0.25941 -0.20358 -0.17044 -0.17654, -0.12515 -0.12270 -0.11960 -0.12516 -0.17300 -0.18489 -0.25941 -0.20358 -0.17044 -0.17654, \n",
      "29k, Loss: -0.12532 -0.12169 -0.11974 -0.12595 -0.17287 -0.18487 -0.26115 -0.20339 -0.16922 -0.17555, -0.12532 -0.12169 -0.11974 -0.12595 -0.17287 -0.18487 -0.26115 -0.20339 -0.16922 -0.17555, \n",
      "30k, Loss: -0.12527 -0.12270 -0.11968 -0.12427 -0.16900 -0.18080 -0.26127 -0.20410 -0.16936 -0.17577, -0.12527 -0.12270 -0.11968 -0.12427 -0.16900 -0.18080 -0.26127 -0.20410 -0.16936 -0.17577, \n",
      "31k, Loss: -0.12537 -0.12253 -0.11978 -0.12530 -0.17210 -0.18367 -0.26302 -0.20374 -0.16915 -0.17534, -0.12537 -0.12253 -0.11978 -0.12530 -0.17210 -0.18367 -0.26302 -0.20374 -0.16915 -0.17534, \n",
      "32k, Loss: -0.12559 -0.12172 -0.11986 -0.12597 -0.17271 -0.18446 -0.26038 -0.20274 -0.16861 -0.17472, -0.12559 -0.12172 -0.11986 -0.12597 -0.17271 -0.18446 -0.26038 -0.20274 -0.16861 -0.17472, \n",
      "33k, Loss: -0.12450 -0.12245 -0.11929 -0.12542 -0.17332 -0.18541 -0.25358 -0.20269 -0.17060 -0.17642, -0.12450 -0.12245 -0.11929 -0.12542 -0.17332 -0.18541 -0.25358 -0.20269 -0.17060 -0.17642, \n",
      "34k, Loss: -0.12562 -0.12225 -0.11988 -0.12571 -0.17246 -0.18455 -0.26201 -0.20351 -0.16946 -0.17604, -0.12562 -0.12225 -0.11988 -0.12571 -0.17246 -0.18455 -0.26201 -0.20351 -0.16946 -0.17604, \n",
      "35k, Loss: -0.12499 -0.12286 -0.11942 -0.12448 -0.17185 -0.18367 -0.26057 -0.20417 -0.17035 -0.17650, -0.12499 -0.12286 -0.11942 -0.12448 -0.17185 -0.18367 -0.26057 -0.20417 -0.17035 -0.17650, \n",
      "36k, Loss: -0.12644 -0.12241 -0.12024 -0.12506 -0.17045 -0.17951 -0.26110 -0.19885 -0.16424 -0.17064, -0.12644 -0.12241 -0.12024 -0.12506 -0.17045 -0.17951 -0.26110 -0.19885 -0.16424 -0.17064, \n",
      "37k, Loss: -0.12607 -0.12238 -0.12008 -0.12549 -0.17235 -0.18384 -0.26315 -0.20293 -0.16773 -0.17437, -0.12607 -0.12238 -0.12008 -0.12549 -0.17235 -0.18384 -0.26315 -0.20293 -0.16773 -0.17437, \n",
      "38k, Loss: -0.12509 -0.12219 -0.11967 -0.12597 -0.17324 -0.18526 -0.25354 -0.20198 -0.17032 -0.17640, -0.12509 -0.12219 -0.11967 -0.12597 -0.17324 -0.18526 -0.25354 -0.20198 -0.17032 -0.17640, \n",
      "39k, Loss: -0.12623 -0.12265 -0.12013 -0.12490 -0.17068 -0.18182 -0.26315 -0.20344 -0.16819 -0.17489, -0.12623 -0.12265 -0.12013 -0.12490 -0.17068 -0.18182 -0.26315 -0.20344 -0.16819 -0.17489, \n",
      "40k, Loss: -0.12599 -0.12237 -0.12003 -0.12509 -0.17012 -0.18194 -0.26254 -0.20369 -0.16929 -0.17611, -0.12599 -0.12237 -0.12003 -0.12509 -0.17012 -0.18194 -0.26254 -0.20369 -0.16929 -0.17611, \n",
      "41k, Loss: -0.12561 -0.12111 -0.11970 -0.12560 -0.17068 -0.18245 -0.26385 -0.20267 -0.16851 -0.17556, -0.12561 -0.12111 -0.11970 -0.12560 -0.17068 -0.18245 -0.26385 -0.20267 -0.16851 -0.17556, \n",
      "42k, Loss: -0.12544 -0.12271 -0.11969 -0.12533 -0.17314 -0.18542 -0.25465 -0.20271 -0.17040 -0.17673, -0.12544 -0.12271 -0.11969 -0.12533 -0.17314 -0.18542 -0.25465 -0.20271 -0.17040 -0.17673, \n",
      "43k, Loss: -0.12609 -0.12280 -0.12005 -0.12513 -0.17264 -0.18426 -0.26122 -0.20389 -0.16994 -0.17635, -0.12609 -0.12280 -0.12005 -0.12513 -0.17264 -0.18426 -0.26122 -0.20389 -0.16994 -0.17635, \n",
      "44k, Loss: -0.12480 -0.12285 -0.11915 -0.12451 -0.17260 -0.18479 -0.25592 -0.20279 -0.17094 -0.17705, -0.12480 -0.12285 -0.11915 -0.12451 -0.17260 -0.18479 -0.25592 -0.20279 -0.17094 -0.17705, \n",
      "45k, Loss: -0.12507 -0.12292 -0.11932 -0.12434 -0.17203 -0.18435 -0.25986 -0.20406 -0.17034 -0.17679, -0.12507 -0.12292 -0.11932 -0.12434 -0.17203 -0.18435 -0.25986 -0.20406 -0.17034 -0.17679, \n",
      "46k, Loss: -0.12544 -0.12255 -0.11973 -0.12572 -0.17276 -0.18513 -0.25180 -0.20143 -0.16999 -0.17666, -0.12544 -0.12255 -0.11973 -0.12572 -0.17276 -0.18513 -0.25180 -0.20143 -0.16999 -0.17666, \n",
      "47k, Loss: -0.12607 -0.12327 -0.11987 -0.12438 -0.17142 -0.18338 -0.26274 -0.20407 -0.16965 -0.17616, -0.12607 -0.12327 -0.11987 -0.12438 -0.17142 -0.18338 -0.26274 -0.20407 -0.16965 -0.17616, \n",
      "48k, Loss: -0.12614 -0.12186 -0.12007 -0.12560 -0.17235 -0.18386 -0.26289 -0.20336 -0.16935 -0.17608, -0.12614 -0.12186 -0.12007 -0.12560 -0.17235 -0.18386 -0.26289 -0.20336 -0.16935 -0.17608, \n",
      "49k, Loss: -0.12625 -0.12326 -0.11999 -0.12469 -0.17233 -0.18418 -0.26124 -0.20387 -0.17031 -0.17658, -0.12625 -0.12326 -0.11999 -0.12469 -0.17233 -0.18418 -0.26124 -0.20387 -0.17031 -0.17658, \n",
      "50k, Loss: -0.12643 -0.12303 -0.12019 -0.12418 -0.16804 -0.17800 -0.26077 -0.20172 -0.16742 -0.17392, -0.12643 -0.12303 -0.12019 -0.12418 -0.16804 -0.17800 -0.26077 -0.20172 -0.16742 -0.17392, \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x1 and 6x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m _loss_step \u001b[38;5;241m=\u001b[39m SOL\u001b[38;5;241m.\u001b[39mtrain_model(data_train\u001b[38;5;241m=\u001b[39mX,data_test\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m     11\u001b[0m                              get_loss\u001b[38;5;241m=\u001b[39mget_loss,optimizer\u001b[38;5;241m=\u001b[39moptimizer,scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m     12\u001b[0m                              n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5e4\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,n_show_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,use_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(),\"savee/model_anaconda3\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, cmap, max_V)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_model\u001b[39m(model,cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrain\u001b[39m\u001b[38;5;124m'\u001b[39m,max_V \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     xx     \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     U_NN   \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     U_NN_min \u001b[38;5;241m=\u001b[39m U_NN\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m      8\u001b[0m     U_NN  \u001b[38;5;241m=\u001b[39m U_NN\u001b[38;5;241m-\u001b[39mU_NN_min\n",
      "Cell \u001b[0;32mIn[19], line 45\u001b[0m, in \u001b[0;36mModel.get_U_np\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_U_np\u001b[39m(\u001b[38;5;28mself\u001b[39m,X): \n\u001b[0;32m---> 45\u001b[0m     U,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_U_dU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mModel.get_U_dU\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_U_dU\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# normalize and ensure x is a tensor\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(X): X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m     U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_U\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     dU \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(U, X, torch\u001b[38;5;241m.\u001b[39mones_like(U), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# dU = dU.T\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m, in \u001b[0;36mFCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(X)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x1 and 6x10)"
     ]
    }
   ],
   "source": [
    "#for choose_id in [1,2,3]:\n",
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)\n",
    "\n",
    "# print(model.mu,model.sigma,model.coef_U)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001))\n",
    "scheduler = None\n",
    "_loss_step = SOL.train_model(data_train=X,data_test=X,\n",
    "                             get_loss=get_loss,optimizer=optimizer,scheduler=scheduler,\n",
    "                             n_steps=int(5e4+1),batch_size=500,n_show_loss=1000,use_tqdm=True)\n",
    "torch.cuda.empty_cache()\n",
    "plot_model(model)\n",
    "# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\n",
    "#torch.save(model.state_dict(),\"savee/model_anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77f2b8b6-9f6d-4fae-ae29-87c4a6cb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"savee/model_99\")\n",
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b07f7a-fb98-4405-8790-18bc4afbc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "# xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "# U_NN     = model.get_U_np(xx)\n",
    "# U_NN_min = U_NN.min()\n",
    "# U_NN     = U_NN-U_NN_min\n",
    "# c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "# ax.legend(fontsize=10)\n",
    "# ax.set_xlabel('$x$',fontsize=10)\n",
    "# ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "# ax.set_xlim([0,2])\n",
    "# ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "# ax.set_xticks([0,.5,1.,1.5,2])\n",
    "# ax.yaxis.grid(linestyle='--')\n",
    "# ax.tick_params(axis=\"both\", labelsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4d82-80a8-422e-a7d9-d2623106808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839674fe",
   "metadata": {},
   "source": [
    "# Visualizing the results for different a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6cc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_models(models):\n",
    "    \n",
    "    xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "    \n",
    "    for k,model_name in enumerate(models):\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        U_NN     = model.get_U_np(xx)\n",
    "        U_NN_min = U_NN.min()\n",
    "        U_NN     = U_NN-U_NN_min\n",
    "        c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5,label=\"$a_{%d}(x)$\"%(k+1))\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlabel('$x$',fontsize=10)\n",
    "    ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "    ax.set_xlim([0,2])\n",
    "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "    ax.set_xticks([0,.5,1.,1.5,2])\n",
    "    ax.yaxis.grid(linestyle='--')\n",
    "    ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_models([\"savee/model_anaconda3\"])\n",
    "#plot_models([\"savee/model_a1_update\", \"savee/model_a2_update\", \"savee/model_a3_update\", \"savee/model_a4\", \"savee/model_a5_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c6ed-2669-4f78-a26d-a70b834146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"/Users/annacoletti/Desktop/savee/model_a3_update.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e3d20-3380-447c-9b87-866c7b207395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as scio\n",
    "# from scipy.io import savemat\n",
    "# scio.savemat('W2_learned_DL', U_NN)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
