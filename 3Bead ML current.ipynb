{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d34f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f3121e-b0e4-46b0-ab2a-09540cec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b280ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def EXIT_NOTEBOOK(): os._exit(00)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ba82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "\n",
    "# # ps\n",
    "# import pysindy as ps\n",
    "\n",
    "# sns.set_theme()\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a3b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 6\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339b9d",
   "metadata": {},
   "source": [
    "# Dataset for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 6)\n"
     ]
    }
   ],
   "source": [
    "from BeadModel import Simulate\n",
    "from SimulationParameters import *\n",
    "\n",
    "X = Simulation(*Simulate(numSims)).positions[:,:,:,0].reshape(-1,nt+1).T\n",
    "# Z = X.view\n",
    "\n",
    "print(X.shape)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbd757",
   "metadata": {},
   "source": [
    "# Set the NN model and Solver with training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606412e9",
   "metadata": {
    "code_folding": [
     2,
     18,
     19,
     28,
     42,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def relu2(X): return func.relu(X)**2\n",
    "def tanh(X): return func.tanh(X)\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=6,num_hidden=2,hidden_dim=10,act=func.tanh,transform=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers  = nn.ModuleList([nn.Linear(input_dim,hidden_dim)])\n",
    "        for _ in range(num_hidden-1): self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.act     = act\n",
    "        self.out     = nn.Linear(hidden_dim,output_dim)\n",
    "        self.transform = transform\n",
    "    def forward(self,X):\n",
    "        if self.transform is not None: X = self.transform(X)\n",
    "        for layer in self.layers: X = self.act(layer(X))\n",
    "        Y = self.out(X)\n",
    "        return Y\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,dim,model_U,unit_len=int(5e3)):\n",
    "        super().__init__()\n",
    "        self.dim      = dim\n",
    "        self.model_U  = model_U\n",
    "        self.unit_len = unit_len\n",
    "        self.mu       = nn.Parameter(torch.tensor([0.]*dim),requires_grad=False) \n",
    "        self.sigma    = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #above two lines should work, but if something doesn't work, check here! Does current self.mu and self.sigma code work if dim>1. Should return a vector since what we're trying to do is calculate mu\n",
    "        #and sigma separately for each imnputted feature. mu is parameter[0] for each row.\n",
    "        self.coef_U   = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #self.mu       = nn.Parameter(torch.tensor([0.]*dim).cuda(),requires_grad=False)\n",
    "        #self.sigma    = nn.Parameter(torch.tensor([1.]*dim).cuda(),requires_grad=False)\n",
    "        #self.coef_U   = nn.Parameter(torch.tensor(1.).cuda(),requires_grad=False)\n",
    "    def get_U_harmonic(self,X): return torch.sum(X**2,axis=-1)\n",
    "        \n",
    "    \n",
    "    def get_U_dU(self,X):\n",
    "        # normalize and ensure x is a tensor\n",
    "        if not torch.is_tensor(X): X = torch.tensor(X, requires_grad=True)\n",
    "        U = self.model_U(X).view(-1)\n",
    "        dU = torch.autograd.grad(U, X, torch.ones_like(U), create_graph=True)[0]\n",
    "        # dU = dU.T\n",
    "        return U,dU\n",
    "\n",
    "    \n",
    "    def get_U_np(self,X): \n",
    "        U,_ = self.get_U_dU(X);\n",
    "        return U.cpu().data.numpy()\n",
    "    \n",
    "    # def get_U_numerical_integration(self,X, dX):\n",
    "    #     outputU = []\n",
    "    #     _,dU = self.get_U_dU(X)\n",
    "    #     # Assume dU has more than two features. for each timestep we'll numerically integrate. current shape is 6x10\n",
    "    #     #Trapezoid rule for numerical integration by each timestep\n",
    "    #     for t in range(dU.shape[1]):\n",
    "    #         currentSum = 0\n",
    "    #         currentDU = dU[:,t]\n",
    "    #         for i in range(len(currentDU)):\n",
    "    #             if i == len(currentDU)-1 or i == 0:\n",
    "    #                 currentSum += currentDU[i] / 2\n",
    "    #             else:\n",
    "    #                 currentSum += currentDU[i]\n",
    "    #         currentSum *= dX\n",
    "    #         outputU.append(currentSum + sum(outputU))\n",
    "    #     U = torch.tensor(outputU)\n",
    "    #     return U\n",
    "\n",
    "    def get_U_numerical_integration(self,X, dX):\n",
    "        U = []\n",
    "        _,dU = self.get_U_dU(X)\n",
    "        # print(dU.shape) #1001 by 6\n",
    "        # Assume dU has more than two features. for each timestep we'll numerically integrate. current shape is 6x10\n",
    "        #Trapezoid rule for numerical integration by each timestep\n",
    "        for t in range(dU.shape[0]):\n",
    "            currentPartialSumofU = 0\n",
    "            currentDU = dU[t,:]\n",
    "            for i in range(len(currentDU)):\n",
    "                if i == len(currentDU)-1 or i == 0:\n",
    "                    currentPartialSumofU += currentDU[i] / 2\n",
    "                else:\n",
    "                    currentPartialSumofU += currentDU[i]\n",
    "            currentU = currentPartialSumofU * dX\n",
    "            U.append(currentU)\n",
    "        \n",
    "        U = torch.tensor(U , requires_grad=False)\n",
    "        # print(U.shape) #shape 6\n",
    "        return U\n",
    "\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "    def train_model(self,data_train,data_test,get_loss,optimizer,\n",
    "                    n_steps,batch_size,scheduler=None,n_show_loss=100,error_model=None,use_tqdm=True):\n",
    "        if use_tqdm: step_range = tqdm(range(n_steps))\n",
    "        else: step_range = range(n_steps)\n",
    "        loss_step = []\n",
    "        for i_step in step_range:\n",
    "            if i_step%n_show_loss==0:\n",
    "                loss_train,loss_test = get_loss(self.model,data_train)[:-1],\\\n",
    "                                       get_loss(self.model,data_test)[:-1]\n",
    "                \n",
    "                def show_num(x): \n",
    "                    if abs(x)<100 and abs(x)>.01: return '%0.5f'%x\n",
    "                    else: return '%0.2e'%x\n",
    "                item1 = '%2dk'%np.int_(i_step/1000)\n",
    "                item2 = 'Loss: '+' '.join([show_num(k) for k in loss_train])\n",
    "                item3 = ' '.join([show_num(k) for k in loss_test])\n",
    "                item4 = ''\n",
    "                if error_model is not None: item4 = 'E(QP): %0.4f' % (error_model(self.model))\n",
    "                # print(', '.join([item1,item2,item3,item4]))\n",
    "                loss_step = loss_step + [i_step] + [k.cpu().data.numpy() for k in loss_train]\\\n",
    "                                                 + [k.cpu().data.numpy() for k in loss_train]\n",
    "            data_batch = data_train[random.sample(range(len(data_train)),\n",
    "                                                  min(batch_size,len(data_train)))]\n",
    "#             print(i_step,data_batch.shape)\n",
    "            loss = get_loss(self.model,data_batch)[-1]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        if error_model is not None: \n",
    "            print(\"Error: %0.5f\" % (error_model(self.model)))\n",
    "        return loss_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5786a635",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_U_dU(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e8e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.mu.shape)\n",
    "# print(model.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82dfe6",
   "metadata": {},
   "source": [
    "# Set the loss function and Train the model for differen a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c4eeaf",
   "metadata": {
    "code_folding": [
     0,
     12,
     16
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0184, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0159, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0195, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0135, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0094, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0108, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0131, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0185, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0269, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0343, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0362, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0349, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0360, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0403, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0335, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0396, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0380, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0411, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0402, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0407, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0396, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0389, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0386, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0403, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0434, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0534, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0501, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0418, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0425, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0379, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0360, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0464, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0531, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0670, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0590, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0509, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0585, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0613, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0569, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0524, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0530, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0453, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0541, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0601, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0599, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0629, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0619, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0632, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0669, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0663, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0655, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0599, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0655, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0540, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0625, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0610, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0578, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0541, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0531, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0587, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0668, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0798, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0757, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0767, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0681, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0623, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0521, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0480, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0437, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0372, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0246, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0222, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0201, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0054, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0091, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0329, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0410, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0345, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0204, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0301, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0380, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0348, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0241, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0138, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0067, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0355, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0274, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0330, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0432, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0435, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0494, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0455, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0442, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0426, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0331, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0319, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0330, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0245, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0290, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0426, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0497, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0497, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0516, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0514, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0484, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0504, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0501, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0519, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0506, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0503, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0533, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0517, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0486, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0501, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0533, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0519, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0521, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0496, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0436, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0433, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0510, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0596, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0572, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0590, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0619, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0658, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0681, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0777, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0677, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0724, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0640, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0660, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0572, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0554, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0535, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0548, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0573, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0543, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0603, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0580, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0560, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0558, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0574, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0517, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0494, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0523, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0473, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0490, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0430, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0498, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0490, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0511, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0553, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0587, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0630, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0828, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0765, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0793, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0832, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0912, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0860, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0825, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0809, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0885, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0830, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0720, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0844, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0987, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0942, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0866, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0885, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0696, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0701, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0720, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0687, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0797, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0802, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0749, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0662, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0593, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0640, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0680, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0694, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0421, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0329, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0258, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0235, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0236, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0295, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0413, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0426, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0395, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0408, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0533, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0538, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0556, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0611, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0621, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0473, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0489, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0439, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0409, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0523, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0558, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0621, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0559, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0492, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0476, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0486, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0462, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0417, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0464, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0468, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0303, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0305, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0334, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0524, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0574, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0494, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0624, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0718, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0848, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0924, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1002, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1099, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1068, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1163, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1074, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0983, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0963, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0752, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0849, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0455, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0409, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0846, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1122, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1166, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1123, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0940, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1007, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0969, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0821, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0773, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0777, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0672, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0548, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0657, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0595, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0630, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0696, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0668, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0658, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0693, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0674, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0643, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0701, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0628, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0410, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0421, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0249, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0602, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0521, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0689, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0726, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0712, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0659, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0572, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0506, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0503, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0439, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0377, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0345, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0372, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0436, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0500, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0240, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0232, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0144, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0275, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0511, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0608, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0600, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0635, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0637, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0646, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0536, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0578, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0615, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0613, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0617, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0641, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0651, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0614, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0672, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0671, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0639, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0676, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0743, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0738, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0657, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0766, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0567, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0473, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0430, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0479, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0682, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0844, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0955, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0906, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0967, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1023, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1007, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0925, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0912, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0905, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0689, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0734, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0753, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0692, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0562, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0443, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0429, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0342, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0640, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0596, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0425, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0446, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0363, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0320, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0209, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0362, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0527, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0681, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0685, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0571, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0601, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0576, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0595, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0595, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0601, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0493, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0471, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0406, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0475, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0334, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0297, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0411, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0387, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0430, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0466, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0550, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0551, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0572, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0575, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0631, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0637, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0677, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0716, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0724, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0791, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0846, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0924, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0815, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0937, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0942, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0966, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0880, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0642, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0573, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0639, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0514, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0508, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0538, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0594, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0560, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0583, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0522, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0511, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0482, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0454, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0406, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0377, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0399, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0417, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0375, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0275, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0272, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0338, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0339, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0317, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0316, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0286, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0350, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0357, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0324, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0309, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0284, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0258, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0302, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0254, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0252, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0165, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0312, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0290, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0286, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0211, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0503, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0478, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0330, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0292, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0189, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0286, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0291, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0421, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0378, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0285, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0083, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0125, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0101, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0054, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0141, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0194, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0244, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0392, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0581, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0662, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0559, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0436, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0437, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0577, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0582, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0438, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0444, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0500, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0539, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0517, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0557, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0532, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0477, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0451, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0435, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0382, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0416, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0363, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0232, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0241, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0123, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0117, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0043, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0063, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0133, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0135, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0245, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0520, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0645, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0821, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0817, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0665, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0401, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0335, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0249, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0217, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0147, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0214, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0171, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0244, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0233, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0327, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0364, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0480, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0568, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0597, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0588, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0651, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0800, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0736, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0660, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0686, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0628, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0615, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0707, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0801, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0680, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0641, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0445, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0341, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0428, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0228, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0326, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0354, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0403, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0313, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0223, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0164, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0125, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0110, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0042, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0091, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0125, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0193, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0028, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0003, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0073, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0171, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0410, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0474, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0574, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0564, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0592, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0631, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0353, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0136, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0146, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0038, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0319, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0621, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0653, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0659, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0218, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0284, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0532, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0444, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0523, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0247, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0283, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0238, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0148, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0094, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0059, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0061, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0062, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0046, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0466, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0384, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0519, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0486, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0439, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0404, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0223, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0013, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0514, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0458, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0322, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0221, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0026, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0066, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0054, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0069, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0042, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0088, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0226, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0177, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0031, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0089, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0193, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0317, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0171, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0150, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0069, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0113, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0255, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0161, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0155, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0090, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0265, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0166, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0219, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0260, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0257, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0264, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0297, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0220, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0154, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0015, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0063, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0023, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0005, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0132, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0099, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0204, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0203, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0131, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0036, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0029, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0062, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0127, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0113, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0032, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0006, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0007, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0014, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0065, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0047, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0013, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0088, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0266, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0358, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0260, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0423, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0295, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0418, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0294, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0325, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0424, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0241, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0141, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0130, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0209, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0143, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0356, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0304, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0170, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0302, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0339, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0498, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0519, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0771, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0702, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0891, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0855, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0430, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0147, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0336, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0302, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0335, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0314, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0266, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0250, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0083, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0118, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0140, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0145, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0140, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0188, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0139, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0029, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0029, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0051, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0026, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0010, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0009, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0124, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0221, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0311, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0322, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0319, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0200, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0284, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0004, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0263, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0398, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0417, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0361, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0393, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0453, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0431, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0380, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0372, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0380, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0382, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0375, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0435, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0427, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0452, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0455, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0464, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0487, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0414, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0481, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0486, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0452, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0467, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0384, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0360, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0381, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0316, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0397, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0336, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0351, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0303, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0205, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0351, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0398, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0433, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0539, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0535, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0651, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0707, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0692, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0675, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0782, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0805, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0788, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0861, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0681, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0640, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0652, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0818, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0955, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0788, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0715, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0807, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0923, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0942, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0824, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0841, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0664, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0678, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0648, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0593, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0335, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0257, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0084, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0034, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0068, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0079, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0167, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0171, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0091, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0147, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0152, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0153, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0225, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0118, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0229, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0545, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0601, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0704, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0802, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0914, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1094, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1180, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1250, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1280, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1370, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1514, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1291, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1275, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1281, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1147, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0919, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0888, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1263, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1295, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1366, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1350, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1313, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1370, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1283, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1037, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0736, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1221, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1185, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0994, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1093, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0857, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0380, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0439, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0260, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1150, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1325, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1304, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1394, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1462, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1120, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0819, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0700, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0709, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0653, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0658, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0628, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0525, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0660, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0620, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0625, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0621, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0689, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0611, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0556, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0517, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0504, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0437, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0374, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0520, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0502, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0530, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0608, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0619, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0496, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0582, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0502, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0247, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0343, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0243, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0238, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0410, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0624, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0685, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0647, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0633, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0648, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0373, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0348, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0169, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0194, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0342, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0436, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0488, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0487, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0466, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0504, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0629, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0668, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0675, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0639, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0787, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0831, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0932, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0972, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0942, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1024, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0969, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0989, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1074, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1160, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1282, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1300, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1134, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1099, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1066, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1047, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1062, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0807, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0618, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0800, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0733, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0426, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0987, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1167, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1134, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1113, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1144, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1255, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1212, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1123, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0895, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0924, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0999, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1041, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1008, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1046, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0692, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0301, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0181, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0116, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0201, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0167, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0149, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0246, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0227, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0713, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0961, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0948, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0919, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0969, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0953, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0828, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0840, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0838, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0934, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0911, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0920, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0855, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0829, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0864, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0899, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0932, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0931, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0947, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0910, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0822, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0793, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0578, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0757, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0571, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0930, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1020, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0871, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0772, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0852, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0846, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0862, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0843, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0826, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0733, grad_fn=<LinalgDetBackward0>),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Loss import getResidue, getAllResidues\n",
    "\n",
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     # xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     # U_NN   = model.get_U_np(xx)\n",
    "#     U_NN   = model.get_U_np(X)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(X[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "    # ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    # plt.show()\n",
    "\n",
    "def plot_model(model,cmap='terrain',max_V = 10,):\n",
    "    xx     = np.linspace(0,2,nt).reshape(-1,1)\n",
    "    U_NN = model.get_U_numerical_integration(X, dX = 1/500)\n",
    "    U_NN = U_NN[:nt]\n",
    "    # U_NN_min = U_NN.min()\n",
    "    # U_NN  = U_NN-U_NN_min\n",
    "    fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "    c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "\n",
    "def get_loss(model,data):\n",
    "    X = data\n",
    "    X = torch.tensor(X).clone().detach().requires_grad_(True)\n",
    "    _,dU = model.get_U_dU(X)\n",
    "    loss = getAllResidues(X, dU)\n",
    "    return loss#,loss\n",
    "\n",
    "\n",
    "# plot_model(model)\n",
    "get_loss(model,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d4b29e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5374e430df24b27b83853d02177f901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.001\u001b[39m))\n\u001b[1;32m      9\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m _loss_step \u001b[38;5;241m=\u001b[39m \u001b[43mSOL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5e4\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_show_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 5e4+1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[7], line 115\u001b[0m, in \u001b[0;36mSolver.train_model\u001b[0;34m(self, data_train, data_test, get_loss, optimizer, n_steps, batch_size, scheduler, n_show_loss, error_model, use_tqdm)\u001b[0m\n\u001b[1;32m    112\u001b[0m             data_batch \u001b[38;5;241m=\u001b[39m data_train[random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_train)),\n\u001b[1;32m    113\u001b[0m                                                   \u001b[38;5;28mmin\u001b[39m(batch_size,\u001b[38;5;28mlen\u001b[39m(data_train)))]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#             print(i_step,data_batch.shape)\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m             loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    117\u001b[0m             loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m _,dU \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_U_dU(X)\n\u001b[0;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mgetAllResidues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/Research/bead-trajectories-quasipotential/Loss.py:75\u001b[0m, in \u001b[0;36mgetAllResidues\u001b[0;34m(x, gradU)\u001b[0m\n\u001b[1;32m     73\u001b[0m     currentX \u001b[38;5;241m=\u001b[39m x[i, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     74\u001b[0m     currentGradU \u001b[38;5;241m=\u001b[39m gradU[i,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     currentResidue \u001b[38;5;241m=\u001b[39m \u001b[43mgetResidue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrentGradU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((currentResidue))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/Research/bead-trajectories-quasipotential/Loss.py:65\u001b[0m, in \u001b[0;36mgetResidue\u001b[0;34m(x, gradU)\u001b[0m\n\u001b[1;32m     62\u001b[0m switchingMatrix[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m switchingMatrix00\n\u001b[1;32m     64\u001b[0m M \u001b[38;5;241m=\u001b[39m advectionMatrix \u001b[38;5;241m+\u001b[39m diffusionMatrix \u001b[38;5;241m+\u001b[39m switchingMatrix\n\u001b[0;32m---> 65\u001b[0m residue \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residue\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for choose_id in [1,2,3]:\n",
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)\n",
    "\n",
    "# print(model.mu,model.sigma,model.coef_U)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001))\n",
    "scheduler = None\n",
    "_loss_step = SOL.train_model(data_train=X,data_test=X,\n",
    "                             get_loss=get_loss,optimizer=optimizer,scheduler=scheduler,\n",
    "                             n_steps=int(5e4+1),batch_size=500,n_show_loss=1000,use_tqdm=True)\n",
    "\n",
    "# 5e4+1\n",
    "torch.cuda.empty_cache()\n",
    "plot_model(model)\n",
    "# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\n",
    "#torch.save(model.state_dict(),\"savee/model_anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77f2b8b6-9f6d-4fae-ae29-87c4a6cb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"savee/model_99\")\n",
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44b07f7a-fb98-4405-8790-18bc4afbc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "# xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "# U_NN     = model.get_U_np(xx)\n",
    "# U_NN_min = U_NN.min()\n",
    "# U_NN     = U_NN-U_NN_min\n",
    "# c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "# ax.legend(fontsize=10)\n",
    "# ax.set_xlabel('$x$',fontsize=10)\n",
    "# ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "# ax.set_xlim([0,2])\n",
    "# ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "# ax.set_xticks([0,.5,1.,1.5,2])\n",
    "# ax.yaxis.grid(linestyle='--')\n",
    "# ax.tick_params(axis=\"both\", labelsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc4b4d82-80a8-422e-a7d9-d2623106808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839674fe",
   "metadata": {},
   "source": [
    "# Visualizing the results for different a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "344c6cc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_models(models):\n",
    "    \n",
    "    xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "    \n",
    "    for k,model_name in enumerate(models):\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        U_NN     = model.get_U_np(xx)\n",
    "        U_NN_min = U_NN.min()\n",
    "        U_NN     = U_NN-U_NN_min\n",
    "        c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5,label=\"$a_{%d}(x)$\"%(k+1))\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlabel('$x$',fontsize=10)\n",
    "    ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "    ax.set_xlim([0,2])\n",
    "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "    ax.set_xticks([0,.5,1.,1.5,2])\n",
    "    ax.yaxis.grid(linestyle='--')\n",
    "    ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_models([\"savee/model_anaconda3\"])\n",
    "#plot_models([\"savee/model_a1_update\", \"savee/model_a2_update\", \"savee/model_a3_update\", \"savee/model_a4\", \"savee/model_a5_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0851750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b79c6ed-2669-4f78-a26d-a70b834146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"/Users/annacoletti/Desktop/savee/model_a3_update.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "319e3d20-3380-447c-9b87-866c7b207395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as scio\n",
    "# from scipy.io import savemat\n",
    "# scio.savemat('W2_learned_DL', U_NN)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
