{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17d34f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66f3121e-b0e4-46b0-ab2a-09540cec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b280ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def EXIT_NOTEBOOK(): os._exit(00)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36ba82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as func\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "\n",
    "# # ps\n",
    "# import pysindy as ps\n",
    "\n",
    "# sns.set_theme()\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9a3b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 6\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339b9d",
   "metadata": {},
   "source": [
    "# Dataset for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "818019ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 6)\n"
     ]
    }
   ],
   "source": [
    "from BeadModel import Simulate\n",
    "from SimulationParameters import *\n",
    "\n",
    "X = Simulation(*Simulate(numSims)).positions[:,:,:,0].reshape(-1,nt+1).T\n",
    "# Z = X.view\n",
    "\n",
    "print(X.shape)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbd757",
   "metadata": {},
   "source": [
    "# Set the NN model and Solver with training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "606412e9",
   "metadata": {
    "code_folding": [
     2,
     18,
     19,
     28,
     42,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def relu2(X): return func.relu(X)**2\n",
    "def tanh(X): return func.tanh(X)\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self,input_dim=6,output_dim=6,num_hidden=2,hidden_dim=10,act=func.tanh,transform=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers  = nn.ModuleList([nn.Linear(input_dim,hidden_dim)])\n",
    "        for _ in range(num_hidden-1): self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.act     = act\n",
    "        self.out     = nn.Linear(hidden_dim,output_dim)\n",
    "        self.transform = transform\n",
    "    def forward(self,X):\n",
    "        if self.transform is not None: X = self.transform(X)\n",
    "        for layer in self.layers: X = self.act(layer(X))\n",
    "        Y = self.out(X)\n",
    "        return Y\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,dim,model_U,unit_len=int(5e3)):\n",
    "        super().__init__()\n",
    "        self.dim      = dim\n",
    "        self.model_U  = model_U\n",
    "        self.unit_len = unit_len\n",
    "        self.mu       = nn.Parameter(torch.tensor([0.]*dim),requires_grad=False) \n",
    "        self.sigma    = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #above two lines should work, but if something doesn't work, check here! Does current self.mu and self.sigma code work if dim>1. Should return a vector since what we're trying to do is calculate mu\n",
    "        #and sigma separately for each imnputted feature. mu is parameter[0] for each row.\n",
    "        self.coef_U   = nn.Parameter(torch.tensor([1.]*dim),requires_grad=False)\n",
    "        #self.mu       = nn.Parameter(torch.tensor([0.]*dim).cuda(),requires_grad=False)\n",
    "        #self.sigma    = nn.Parameter(torch.tensor([1.]*dim).cuda(),requires_grad=False)\n",
    "        #self.coef_U   = nn.Parameter(torch.tensor(1.).cuda(),requires_grad=False)\n",
    "    def get_U_harmonic(self,X): return torch.sum(X**2,axis=-1)\n",
    "        \n",
    "    \n",
    "    def get_U_dU(self,X):\n",
    "        # normalize and ensure x is a tensor\n",
    "        if not torch.is_tensor(X): X = torch.tensor(X, requires_grad=True)\n",
    "        U = self.model_U(X).view(-1)\n",
    "        dU = torch.autograd.grad(U, X, torch.ones_like(U), create_graph=True)[0]\n",
    "        # dU = dU.T\n",
    "        return U,dU\n",
    "\n",
    "    \n",
    "    def get_U_np(self,X): \n",
    "        U,_ = self.get_U_dU(X);\n",
    "        return U.cpu().data.numpy()\n",
    "    \n",
    "    # def get_U_numerical_integration(self,X, dX):\n",
    "    #     outputU = []\n",
    "    #     _,dU = self.get_U_dU(X)\n",
    "    #     # Assume dU has more than two features. for each timestep we'll numerically integrate. current shape is 6x10\n",
    "    #     #Trapezoid rule for numerical integration by each timestep\n",
    "    #     for t in range(dU.shape[1]):\n",
    "    #         currentSum = 0\n",
    "    #         currentDU = dU[:,t]\n",
    "    #         for i in range(len(currentDU)):\n",
    "    #             if i == len(currentDU)-1 or i == 0:\n",
    "    #                 currentSum += currentDU[i] / 2\n",
    "    #             else:\n",
    "    #                 currentSum += currentDU[i]\n",
    "    #         currentSum *= dX\n",
    "    #         outputU.append(currentSum + sum(outputU))\n",
    "    #     U = torch.tensor(outputU)\n",
    "    #     return U\n",
    "\n",
    "    def get_U_numerical_integration(self,X, dX):\n",
    "        U = []\n",
    "        _,dU = self.get_U_dU(X)\n",
    "        # Assume dU has more than two features. for each timestep we'll numerically integrate. current shape is 6x10\n",
    "        #Trapezoid rule for numerical integration by each timestep\n",
    "        for t in range(dU.shape[1]):\n",
    "            currentPartialSumofU = 0\n",
    "            currentDU = dU[:,t]\n",
    "            for i in range(len(currentDU)):\n",
    "                if i == len(currentDU)-1 or i == 0:\n",
    "                    currentPartialSumofU += currentDU[i] / 2\n",
    "                else:\n",
    "                    currentPartialSumofU += currentDU[i]\n",
    "            currentU = currentPartialSumofU * dX\n",
    "            U.append(currentU)\n",
    "            \n",
    "        # print(U.shape)\n",
    "        return U\n",
    "\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "    def train_model(self,data_train,data_test,get_loss,optimizer,\n",
    "                    n_steps,batch_size,scheduler=None,n_show_loss=100,error_model=None,use_tqdm=True):\n",
    "        if use_tqdm: step_range = tqdm(range(n_steps))\n",
    "        else: step_range = range(n_steps)\n",
    "        loss_step = []\n",
    "        for i_step in step_range:\n",
    "            if i_step%n_show_loss==0:\n",
    "                loss_train,loss_test = get_loss(self.model,data_train)[:-1],\\\n",
    "                                       get_loss(self.model,data_test)[:-1]\n",
    "                \n",
    "                def show_num(x): \n",
    "                    if abs(x)<100 and abs(x)>.01: return '%0.5f'%x\n",
    "                    else: return '%0.2e'%x\n",
    "                item1 = '%2dk'%np.int_(i_step/1000)\n",
    "                item2 = 'Loss: '+' '.join([show_num(k) for k in loss_train])\n",
    "                item3 = ' '.join([show_num(k) for k in loss_test])\n",
    "                item4 = ''\n",
    "                if error_model is not None: item4 = 'E(QP): %0.4f' % (error_model(self.model))\n",
    "                print(', '.join([item1,item2,item3,item4]))\n",
    "                loss_step = loss_step + [i_step] + [k.cpu().data.numpy() for k in loss_train]\\\n",
    "                                                 + [k.cpu().data.numpy() for k in loss_train]\n",
    "            data_batch = data_train[random.sample(range(len(data_train)),\n",
    "                                                  min(batch_size,len(data_train)))]\n",
    "#             print(i_step,data_batch.shape)\n",
    "            loss = get_loss(self.model,data_batch)[-1]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        if error_model is not None: \n",
    "            print(\"Error: %0.5f\" % (error_model(self.model)))\n",
    "        return loss_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5786a635",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd4f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_U_dU(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94e8e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.mu.shape)\n",
    "# print(model.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5df5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     U_NN   = model.get_U_np(xx)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(xx[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", labelsize=10)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82dfe6",
   "metadata": {},
   "source": [
    "# Set the loss function and Train the model for differen a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69c4eeaf",
   "metadata": {
    "code_folding": [
     0,
     12,
     16
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0161, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0186, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0145, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0209, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0198, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0190, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0180, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0233, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0179, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0218, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0236, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0233, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0283, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0309, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0360, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0332, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0320, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0348, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0338, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0343, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0427, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0333, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0416, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0457, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0513, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0467, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0531, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0602, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0594, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0490, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0465, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0535, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0584, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0814, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0845, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0972, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0926, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0950, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0841, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0831, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1012, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1055, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1104, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1176, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0977, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0856, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1013, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0932, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1036, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1137, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1046, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1157, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1249, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1277, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1118, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1030, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1143, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1122, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1220, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1307, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1351, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1215, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1394, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1498, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1463, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1395, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1446, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1553, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1550, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1538, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1515, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1549, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1605, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1548, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1538, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1538, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1519, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1512, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1523, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1567, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1552, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1687, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1558, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1546, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1639, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1687, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1540, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1372, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0938, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0996, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1098, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1161, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1231, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1218, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1329, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1381, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1342, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1550, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1675, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1609, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1542, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1485, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1586, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1565, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1574, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1694, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1718, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1630, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1503, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1665, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1722, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1748, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1775, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1819, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1777, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1849, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1744, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2098, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2118, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2267, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1992, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1417, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1309, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1574, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1306, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1298, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1360, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1526, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1738, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2038, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2135, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2035, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1753, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2086, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2006, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2053, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2082, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1967, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1884, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1682, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2038, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1556, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1577, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1479, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1780, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1494, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1412, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1710, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1660, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1291, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0977, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0807, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0644, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0752, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0942, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0683, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0686, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0896, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1009, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1018, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0953, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0878, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0885, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0867, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1040, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1140, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1180, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1334, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1466, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1758, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1755, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1611, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1677, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1706, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1684, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1890, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1767, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1879, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1846, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1696, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1620, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1687, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1722, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1706, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1776, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1796, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1884, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1722, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1743, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1661, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1476, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1460, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1527, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1599, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1647, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1625, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1555, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1689, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1617, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1477, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1686, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1653, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1710, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1578, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1459, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1308, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1250, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1300, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1072, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1051, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1009, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0748, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0762, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0924, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0927, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0888, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0877, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0691, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0884, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1090, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1198, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1246, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1307, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1292, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1152, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1098, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1056, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1162, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1073, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1319, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1303, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1396, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1395, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1391, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1398, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1562, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1531, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1605, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1662, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1765, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1771, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1874, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1956, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2209, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2272, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2068, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2153, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2195, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2188, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2183, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2272, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2099, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2213, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2042, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.2104, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1857, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1642, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1482, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1375, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1583, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1292, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1217, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1293, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0947, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1060, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0648, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1413, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1652, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1473, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1058, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1100, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1355, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1542, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1436, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1070, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0522, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0564, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0481, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0196, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0863, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1409, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1553, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1384, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1268, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1418, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1241, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1325, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1323, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1415, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1200, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1212, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1266, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1231, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1569, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1492, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1700, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1401, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1619, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1451, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1493, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1496, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1396, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1520, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1738, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1569, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1260, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1031, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1276, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1372, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1188, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1140, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1011, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0962, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1073, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0818, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0997, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0981, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0968, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0925, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0954, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0892, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0945, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0915, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0963, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1072, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1080, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0865, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0640, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0623, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0656, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0705, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0774, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0612, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0743, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0838, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0790, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0926, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1115, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1215, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1319, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1546, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1552, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1658, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1516, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1371, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1232, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1112, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0989, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0970, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1084, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1196, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1170, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1450, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1199, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1507, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1418, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1248, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1332, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1231, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1321, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1226, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1258, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1127, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1035, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0929, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0854, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0672, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0829, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0597, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0491, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0423, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0667, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0846, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0754, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0852, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0856, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0833, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0791, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0947, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0887, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1005, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0965, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0879, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0978, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0926, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0936, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0794, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0679, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0767, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0881, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0950, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1029, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0948, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1030, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1010, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0880, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0801, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0751, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0722, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0708, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0753, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0597, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0468, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0515, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0447, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0197, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0038, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0193, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0004, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0284, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0546, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0411, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0284, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0050, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0030, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0008, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0038, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0037, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0017, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0006, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0080, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0080, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0118, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0105, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0025, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0377, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0591, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0708, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0785, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0830, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0734, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0817, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0802, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0857, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1017, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0921, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1011, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0813, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1098, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1168, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0935, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0936, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0789, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0891, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0829, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0939, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0916, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0749, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0716, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0614, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0693, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0675, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0685, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0593, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0751, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0691, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0539, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0627, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0688, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0703, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0808, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0960, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1092, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1004, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1089, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0925, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0749, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1115, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1043, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0941, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0896, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0908, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0868, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0741, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0564, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0681, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0616, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0606, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0729, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0541, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0617, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0646, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0694, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1012, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1045, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1001, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1020, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0807, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0805, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1086, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1018, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1071, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1158, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0968, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0974, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0949, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0971, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1119, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1098, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1008, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1085, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1188, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1293, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1115, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1187, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1199, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1247, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1135, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1148, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1102, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1006, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1014, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1044, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1174, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1219, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1169, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1218, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1301, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1357, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1233, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1046, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1068, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.1036, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0836, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0728, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0815, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0616, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0686, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0633, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0426, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0592, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0654, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0497, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0688, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0444, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0328, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0301, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0313, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0327, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0300, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0323, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0327, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0314, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0283, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0398, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0124, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0189, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0254, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0462, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0057, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0077, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0065, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0002, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0108, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0106, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0153, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0115, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0023, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0052, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0065, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0044, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0140, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0124, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0063, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0072, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0059, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0255, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0153, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0043, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0010, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0004, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0027, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0013, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0028, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0195, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0177, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0186, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0304, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0488, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0573, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0534, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0570, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0651, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0659, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0761, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0685, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0708, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0664, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0641, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0659, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0610, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0598, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0453, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0464, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0307, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0457, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0469, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0190, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0457, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0021, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0255, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0226, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0192, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0045, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0076, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0133, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0086, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0080, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0085, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0036, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0043, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0015, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0214, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0290, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0169, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0179, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0026, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0030, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0149, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0078, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0042, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0339, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0321, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0239, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0224, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0171, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0194, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0148, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0149, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0150, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0078, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0022, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0032, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0056, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0009, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0081, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0030, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0012, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0101, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0090, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0148, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0129, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0037, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0050, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0106, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0122, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0155, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0147, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0182, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0123, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0105, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0045, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0033, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0076, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0134, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0019, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(2.4278e-05, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0022, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0138, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0172, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0223, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0257, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0373, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0347, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0477, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0471, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0478, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0609, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0669, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0644, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0499, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0406, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0396, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0349, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0320, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0368, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0166, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0049, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0015, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0029, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0018, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0026, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0123, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0127, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0073, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0014, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0073, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0093, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0083, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0130, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0086, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0005, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0095, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(0.0025, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0090, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0115, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0208, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0362, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0461, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0590, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0654, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0691, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0625, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0585, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0545, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0539, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0548, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0650, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0662, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0651, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0619, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0631, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0509, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0667, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0674, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0577, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0487, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0510, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0512, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0620, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0566, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0399, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0387, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0421, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0392, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0502, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0439, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0419, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0388, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0349, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0322, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0267, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0272, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0337, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0342, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0269, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0253, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0235, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0126, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0164, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0230, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0322, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0355, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0293, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0369, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0527, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0465, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0412, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0315, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0369, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0483, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0465, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0314, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0150, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0111, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0160, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0165, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0245, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0178, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0341, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0337, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0352, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0388, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0485, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0484, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0500, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0443, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0402, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0469, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0341, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0222, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0116, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0330, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0661, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0765, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0763, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0754, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0719, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0655, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0706, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0743, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0756, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0777, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0762, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0739, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0758, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0775, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0762, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0705, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0787, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0805, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0802, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0772, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0695, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0547, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0695, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0621, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0558, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0493, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0409, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0376, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0251, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0159, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0121, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0064, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0042, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0052, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0063, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0077, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0077, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0285, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0215, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0166, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0210, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0480, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0437, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0383, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0468, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0501, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0429, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0434, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0448, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0398, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0322, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0286, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0240, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0489, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0404, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0446, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0442, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0431, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0410, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0418, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0329, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0340, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0329, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0321, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0327, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0281, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0298, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0291, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0402, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0400, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0401, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0344, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0312, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0285, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0269, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0234, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0471, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0430, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0415, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0313, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0290, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0278, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0259, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0234, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0254, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0318, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0367, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0305, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0256, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0271, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0274, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0337, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0545, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0536, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0505, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0546, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0449, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0548, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0534, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0600, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0469, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0494, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0464, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0421, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0424, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0474, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0512, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0564, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0601, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0586, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0569, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0634, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0628, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0645, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0709, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0665, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0650, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0649, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0604, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0652, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0520, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0535, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0533, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0486, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0465, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0443, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0452, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0434, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0477, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0473, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0500, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0495, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0478, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0447, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0515, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0520, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0525, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0522, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0535, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0544, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0528, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0516, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0518, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0417, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0385, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0406, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0354, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0367, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0452, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0381, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0414, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0348, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0326, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0405, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0390, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0481, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0536, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0539, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0579, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0659, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0622, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0537, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0602, grad_fn=<LinalgDetBackward0>),\n",
       " tensor(-0.0587, grad_fn=<LinalgDetBackward0>),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Loss import getResidue, getAllResidues\n",
    "\n",
    "# def plot_model(model,cmap='terrain',max_V = 10):\n",
    "    \n",
    "#     # xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "#     # U_NN   = model.get_U_np(xx)\n",
    "#     U_NN   = model.get_U_np(X)\n",
    "#     U_NN_min = U_NN.min()\n",
    "#     U_NN  = U_NN-U_NN_min\n",
    "#     fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "#     c      = ax.plot(X[:,0],U_NN,'-',lw=1.5,)\n",
    "\n",
    "    # ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    # plt.show()\n",
    "\n",
    "def plot_model(model,cmap='terrain',max_V = 10,):\n",
    "    xx     = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    U_NN = model.get_U_numerical_integration(X, dX = 1/500)\n",
    "    # U_NN_min = U_NN.min()\n",
    "    # U_NN  = U_NN-U_NN_min\n",
    "    fig, ax    = plt.subplots(1,1,figsize=(5,3),dpi=200,constrained_layout=True)\n",
    "    c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "\n",
    "def get_loss(model,data):\n",
    "    X = data\n",
    "    X = torch.tensor(X).clone().detach().requires_grad_(True)\n",
    "    _,dU = model.get_U_dU(X)\n",
    "    loss = getAllResidues(X, dU)\n",
    "    return loss#,loss\n",
    "\n",
    "\n",
    "# plot_model(model)\n",
    "get_loss(model,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65d4b29e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699e74cb77554a5c9c9c2873660c22cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0k, Loss: -0.06405 -0.05629 -0.05089 -0.05445 -0.05546 -0.05624 -0.06339 -0.06383 -0.05664 -0.05415 -0.05210 -0.05067 -0.06069 -0.05893 -0.05776 -0.06348 -0.07553 -0.07605 -0.08083 -0.10453 -0.10563 -0.08941 -0.09551 -0.08120 -0.08991 -0.08628 -0.08130 -0.06005 -0.05936 -0.05698 -0.06096 -0.06246 -0.06503 -0.06490 -0.07351 -0.08963 -0.09469 -0.09882 -0.09099 -0.10573 -0.08691 -0.09183 -0.08959 -0.10199 -0.10472 -0.08536 -0.09432 -0.09683 -0.08333 -0.09452 -0.10054 -0.10212 -0.11329 -0.11099 -0.11188 -0.11594 -0.10082 -0.09070 -0.09725 -0.11158 -0.10273 -0.10852 -0.12575 -0.11260 -0.13266 -0.17185 -0.16909 -0.20037 -0.15598 -0.18963 -0.19653 -0.15738 -0.12545 -0.12180 -0.13325 -0.17073 -0.15872 -0.17753 -0.17235 -0.16998 -0.17204 -0.19313 -0.21608 -0.24767 -0.20422 -0.20957 -0.18652 -0.21460 -0.22147 -0.20553 -0.22445 -0.25456 -0.27129 -0.24851 -0.26544 -0.21605 -0.20688 -0.19754 -0.22356 -0.21337 -0.18743 -0.17224 -0.12988 -0.13611 -0.14783 -0.14963 -0.13299 -0.14645 -0.18567 -0.20314 -0.18523 -0.18416 -0.16739 -0.17276 -0.17252 -0.19055 -0.19308 -0.18714 -0.23199 -0.20951 -0.20847 -0.16667 -0.14130 -0.16389 -0.15723 -0.16721 -0.17687 -0.20160 -0.21917 -0.24716 -0.20183 -0.19244 -0.21417 -0.24511 -0.26883 -0.26202 -0.25934 -0.26277 -0.24290 -0.22324 -0.16571 -0.12489 -0.10372 -0.06817 -0.07978 -0.03185 0.02894 -0.04848 -0.05898 -0.06997 -0.11536 -0.11066 -0.14159 -0.09992 -0.05285 -0.10851 -0.14905 -0.11727 -0.16026 -0.13950 -0.11643 -0.11058 -0.08618 -0.05732 8.76e-03 -0.11781 -0.07077 -0.13118 -0.11744 -0.21782 -0.19523 -0.14247 -0.10357 -0.03951 -0.03025 -0.07061 -0.03634 -0.08465 -0.11675 -0.04343 -0.01101 -0.03092 -0.03385 0.03353 -0.08932 -0.15153 -0.16304 -0.16228 -0.15319 -0.14086 -0.13668 -0.13595 -0.16365 -0.16605 -0.20317 -0.23702 -0.23610 -0.23063 -0.25581 -0.27158 -0.27090 -0.30432 -0.29887 -0.29036 -0.26370 -0.23875 -0.24036 -0.24669 -0.21357 -0.24190 -0.24800 -0.27577 -0.25156 -0.24472 -0.19555 -0.14816 -0.18928 -0.18349 -0.19723 -0.19460 -0.20336 -0.17456 -0.18780 -0.17880 -0.18911 -0.20845 -0.20428 -0.19719 -0.17662 -0.16194 -0.12775 -0.11876 -0.12664 -0.10430 -0.11862 -0.11684 -0.11860 -0.11712 -0.11391 -0.10287 -0.09081 -0.09224 -0.09319 -0.09427 -0.09679 -0.09565 -0.09266 -0.10459 -0.10206 -0.09469 -0.09301 -0.08666 -0.07783 -0.10395 -0.11801 -0.11932 -0.13966 -0.12230 -0.12342 -0.15560 -0.16403 -0.17900 -0.20310 -0.25196 -0.27828 -0.29024 -0.33610 -0.31562 -0.33692 -0.25907 -0.24879 -0.20542 -0.30642 -0.23422 -0.24080 -0.23805 -0.24764 -0.16755 -0.31484 -0.21897 -0.31486 -0.25441 -0.31849 -0.31480 -0.31527 -0.28724 -0.26799 -0.22513 -0.19348 -0.20415 -0.25581 -0.26543 -0.27212 -0.24278 -0.27217 -0.26717 -0.23336 -0.27033 -0.25019 -0.23787 -0.18458 -0.21078 -0.18628 -0.22139 -0.21083 -0.21548 -0.22256 -0.23717 -0.24373 -0.22250 -0.17765 -0.13893 -0.13893 -0.15496 -0.15951 -0.18788 -0.18331 -0.19824 -0.16923 -0.17186 -0.19059 -0.22090 -0.22686 -0.28440 -0.25613 -0.27206 -0.23698 -0.22436 -0.23689 -0.23894 -0.28381 -0.29970 -0.32952 -0.34526 -0.29413 -0.24753 -0.22536 -0.24983 -0.29188 -0.28425 -0.28328 -0.22254 -0.19942 -0.16942 -0.19071 -0.22117 -0.18794 -0.20649 -0.19079 -0.16780 -0.17370 -0.16425 -0.15571 -0.14304 -0.12140 -0.10847 -0.13202 -0.15789 -0.15211 -0.13961 -0.13412 -0.11919 -0.14426 -0.13587 -0.13357 -0.12791 -0.12565 -0.10807 -0.09930 -0.09386 -0.11263 -0.12113 -0.13815 -0.14321 -0.16193 -0.17177 -0.14212 -0.12620 -0.11934 -0.11290 -0.11627 -0.12969 -0.14770 -0.14652 -0.16101 -0.18403 -0.16904 -0.15928 -0.16996 -0.12972 -0.13802 -0.12005 -0.13097 -0.13649 -0.11978 -0.11253 -0.10597 -0.11091 -0.11033 -0.10910 -0.10160 -0.10123 -0.10298 -0.10583 -0.09811 -0.09156 -0.09240 -0.09924 -0.09970 -0.09614 -0.09953 -0.09390 -0.09133 -0.08979 -0.09443 -0.09376 -0.09950 -0.09751 -0.10522 -0.12085 -0.11736 -0.12292 -0.13317 -0.12567 -0.13654 -0.12618 -0.11773 -0.11673 -0.11780 -0.12382 -0.11912 -0.13101 -0.12548 -0.13375 -0.13813 -0.15071 -0.13651 -0.14172 -0.14765 -0.14220 -0.13573 -0.11923 -0.12373 -0.13875 -0.13037 -0.15494 -0.14163 -0.14531 -0.14212 -0.12736 -0.12497 -0.10931 -0.09926 -0.05894 -0.04269 -0.02581 -2.04e-03 -0.02671 0.01968 -0.03927 -0.11375 -0.10334 -0.12465 -0.14743 -0.14967 -0.13906 -0.11129 -0.11235 -0.10955 -0.09881 -0.10040 -0.07057 -0.02077 -9.13e-03 -0.02596 -0.04497 -0.05721 0.03156 0.02850 -0.05257 -0.04090 -0.07030 -0.02676 -0.04100 8.51e-03 2.24e-04 -0.05238 -0.05489 -0.14500 -0.10393 -0.10164 -0.03653 -0.08814 -0.12957 -0.14558 -0.13158 -0.15670 -0.15367 -0.14815 -0.14422 -0.13516 -0.12061 -0.12110 -0.07824 -0.07933 -0.01294 6.16e-03 0.01273 -0.09188 -0.05016 -0.06141 0.05084 0.02692 0.02881 -0.02692 -0.04946 -0.09568 -0.10906 -0.08371 -0.11179 -0.10888 -0.10170 -0.09048 -0.07550 -0.08042 -0.07430 -0.07728 -0.07378 -0.02014 0.05129 -0.06523 -0.10861 -0.09347 -0.08750 -0.06036 -0.01243 0.01437 2.34e-03 -1.88e-03 -0.04813 -0.04296 -0.07711 -0.06025 -0.06574 -0.10476 -0.11079 -0.12548 -0.11303 -0.10668 -0.09430 -0.12501 -0.11089 -0.12555 -0.12131 -0.11829 -0.06574 -0.08763 -0.04695 -0.09618 -0.05458 -0.05742 -0.04818 -0.03951 0.02643 0.10353 0.12197 0.24953 0.14606 0.21858 0.16028 0.21780 0.19999 0.06692 0.09833 0.09287 0.17018 0.10774 0.22905 0.22816 0.26747 0.17504 0.23662 0.23097 0.18574 0.13375 0.07835 0.05417 0.08113 0.06532 0.07492 0.03228 -0.19631 -0.09504 -0.07014 -0.01836 -0.19376 -0.15046 -0.24297 -0.21364 -0.26679 -0.26302 -0.28448 -0.25962 -0.25281 -0.21648 -0.21549 -0.21998 -0.26732 -0.26340 -0.26590 -0.24902 -0.26692 -0.24837 -0.24823 -0.24818 -0.23184 -0.25104 -0.23687 -0.20344 -0.19670 -0.14577 -0.12912 -0.08645 -0.04075 -0.01046 -0.01266 -6.15e-03 0.01121 8.03e-03 5.31e-03 -0.01190 -5.22e-03 -8.26e-04 2.29e-03 6.21e-03 4.39e-04 7.14e-03 6.02e-03 0.01608 0.01257 0.01106 0.01359 5.26e-03 2.83e-03 0.01429 3.43e-03 -0.02494 -0.11889 -0.21211 -0.17674 -0.18685 -0.20712 -0.22221 -0.19496 -0.18219 -0.16759 -0.14235 -0.13876 -0.16096 -0.14747 -0.15912 -0.18675 -0.18461 -0.18789 -0.19592 -0.20266 -0.20468 -0.20142 -0.18675 -0.22216 -0.20061 -0.20047 -0.19799 -0.17476 -0.17754 -0.18515 -0.19817 -0.18964 -0.21002 -0.21943 -0.19871 -0.20396 -0.19776 -0.21884 -0.25046 -0.25589 -0.27602 -0.27051 -0.27789 -0.27678 -0.23028 -0.22764 -0.21955 -0.21001 -0.22550 -0.25381 -0.25943 -0.22142 -0.19288 -0.19552 -0.23800 -0.20451 -0.21355 -0.17454 -0.11622 -0.10425 -0.02415 -0.10679 -0.14636 -0.19367 -0.09151 -0.12104 -0.06877 -0.13256 -0.08316 -0.05165 0.01140 -9.48e-03 0.02467 0.01525 -0.02266 -7.35e-03 5.26e-03 0.03432 0.02604 0.02501 -0.03582 -0.04544 -0.11687 -0.07871 -0.07815 -0.06481 -0.06157 -0.07612 -0.07264 -0.07632 -0.08048 -0.07472 -0.12085 -0.09617 -0.15608 -0.14028 -0.13534 -0.12912 -0.11270 -0.11413 -0.10163 -0.07038 -0.13040 -0.15175 -0.14938 -0.13624 -0.12437 -0.12231 -0.10512 -0.09801 -0.09507 -0.09678 -0.09215 -0.07508 -0.05659 -0.05484 -0.06264 -0.07909 -0.08608 -0.08861 -0.09177 -0.08217 -0.07122 -0.08803 -0.08411 -0.08964 -0.08410 -0.09307 -0.08595 -0.10700 -0.10991 -0.12035 -0.11374 -0.11536 -0.11955 -0.09977 -0.09545 -0.07767 -0.07479 -0.06459 -0.05243 -0.05476 -0.05628 -0.06754 -0.07109 -0.06780 -0.07800 -0.06557 -0.06917 -0.07323 -0.07204 -0.07442 -0.07529 -0.06581 -0.06384 -0.08744 -0.10276 -0.09867 -0.10865 -0.12294 -0.12888 -0.10241 -0.09630 -0.10826 -0.13090 -0.13999 -0.14994 -0.12918 -0.14977 -0.14188 -0.16125 -0.16435 -0.14587 -0.13571 -0.13586 -0.11712 -0.10275 -0.09929 -0.08472 -0.09293 -0.08486 -0.07446 -0.07700 -0.05884 -0.04414 -0.03847 -0.05548 -0.08085 -0.09066 -0.09031 -0.08162 -0.07842 -0.07024 -0.07763 -0.08264 -0.08557 -0.08885 -0.08732 -0.08866 -0.09134 -0.08704 -0.08471 -0.07976 -0.09389 -0.09127 -0.08914 -0.08872 -0.08706 -0.07180 -0.08256 -0.07608 -0.07295 -0.06743 -0.07307 -0.07053 -0.05082 -0.03798 -0.03571 -0.03493 -0.03595 -0.02416 -0.02887 -0.02843 -0.02701 -0.05883 -0.04628 -0.03604 -0.04167 -0.07571 -0.06790 -0.06174 -0.05879 -0.06949 -0.07459 -0.06091 -0.06181 -0.06946 -0.04416 -0.02091 -0.01793 -0.01464 -0.08453 -0.06450 -0.06664 -0.06973 -0.06699 -0.06406 -0.06873 -0.03257 -0.03945 -0.03052 -0.03158 -0.02617 -0.01700 -0.03397 -0.02936 -0.07489 -0.07390 -0.07588 -0.03891 -0.02788 -0.02839 -0.02707 -0.03330 -0.07836 -0.07528 -0.06856 -0.06722 -0.05247 -0.04758 -0.04385 -0.04201 -0.03656 -0.03051 -0.03981 -0.04001 -0.02067 -0.01990 -0.02600 -0.01722 -0.04040 -0.12004 -0.13970 -0.17256 -0.17397 -0.18614 -0.20930 -0.21500 -0.23691 -0.22669 -0.22687 -0.22805 -0.23156 -0.20501 -0.17858 -0.13677 -0.10626 -0.04475 -0.16951 -0.28751 -0.28298 -0.33466 -0.35308 -0.36036 -0.34900 -0.33648 -0.33233 -0.31891 -0.33602 -0.29271 -0.30841 -0.30705 -0.29241 -0.29454 -0.30228 -0.29214 -0.23737 -0.22790 -0.23550 -0.23552 -0.22697 -0.21182 -0.27285 -0.26227 -0.33232 -0.31886 -0.29225 -0.28778 -0.29967 -0.28761 -0.32674 -0.34162 -0.33113 -0.26894 -0.23825 -0.24049 -0.24553 -0.24888 -0.27612 -0.26568 -0.27739 -0.26126 -0.25677 -0.28314 -0.28091 -0.31292 -0.33661 -0.34658 -0.36053 -0.38754 -0.39832 -0.36143 -0.39784 -0.35608, -0.06405 -0.05629 -0.05089 -0.05445 -0.05546 -0.05624 -0.06339 -0.06383 -0.05664 -0.05415 -0.05210 -0.05067 -0.06069 -0.05893 -0.05776 -0.06348 -0.07553 -0.07605 -0.08083 -0.10453 -0.10563 -0.08941 -0.09551 -0.08120 -0.08991 -0.08628 -0.08130 -0.06005 -0.05936 -0.05698 -0.06096 -0.06246 -0.06503 -0.06490 -0.07351 -0.08963 -0.09469 -0.09882 -0.09099 -0.10573 -0.08691 -0.09183 -0.08959 -0.10199 -0.10472 -0.08536 -0.09432 -0.09683 -0.08333 -0.09452 -0.10054 -0.10212 -0.11329 -0.11099 -0.11188 -0.11594 -0.10082 -0.09070 -0.09725 -0.11158 -0.10273 -0.10852 -0.12575 -0.11260 -0.13266 -0.17185 -0.16909 -0.20037 -0.15598 -0.18963 -0.19653 -0.15738 -0.12545 -0.12180 -0.13325 -0.17073 -0.15872 -0.17753 -0.17235 -0.16998 -0.17204 -0.19313 -0.21608 -0.24767 -0.20422 -0.20957 -0.18652 -0.21460 -0.22147 -0.20553 -0.22445 -0.25456 -0.27129 -0.24851 -0.26544 -0.21605 -0.20688 -0.19754 -0.22356 -0.21337 -0.18743 -0.17224 -0.12988 -0.13611 -0.14783 -0.14963 -0.13299 -0.14645 -0.18567 -0.20314 -0.18523 -0.18416 -0.16739 -0.17276 -0.17252 -0.19055 -0.19308 -0.18714 -0.23199 -0.20951 -0.20847 -0.16667 -0.14130 -0.16389 -0.15723 -0.16721 -0.17687 -0.20160 -0.21917 -0.24716 -0.20183 -0.19244 -0.21417 -0.24511 -0.26883 -0.26202 -0.25934 -0.26277 -0.24290 -0.22324 -0.16571 -0.12489 -0.10372 -0.06817 -0.07978 -0.03185 0.02894 -0.04848 -0.05898 -0.06997 -0.11536 -0.11066 -0.14159 -0.09992 -0.05285 -0.10851 -0.14905 -0.11727 -0.16026 -0.13950 -0.11643 -0.11058 -0.08618 -0.05732 8.76e-03 -0.11781 -0.07077 -0.13118 -0.11744 -0.21782 -0.19523 -0.14247 -0.10357 -0.03951 -0.03025 -0.07061 -0.03634 -0.08465 -0.11675 -0.04343 -0.01101 -0.03092 -0.03385 0.03353 -0.08932 -0.15153 -0.16304 -0.16228 -0.15319 -0.14086 -0.13668 -0.13595 -0.16365 -0.16605 -0.20317 -0.23702 -0.23610 -0.23063 -0.25581 -0.27158 -0.27090 -0.30432 -0.29887 -0.29036 -0.26370 -0.23875 -0.24036 -0.24669 -0.21357 -0.24190 -0.24800 -0.27577 -0.25156 -0.24472 -0.19555 -0.14816 -0.18928 -0.18349 -0.19723 -0.19460 -0.20336 -0.17456 -0.18780 -0.17880 -0.18911 -0.20845 -0.20428 -0.19719 -0.17662 -0.16194 -0.12775 -0.11876 -0.12664 -0.10430 -0.11862 -0.11684 -0.11860 -0.11712 -0.11391 -0.10287 -0.09081 -0.09224 -0.09319 -0.09427 -0.09679 -0.09565 -0.09266 -0.10459 -0.10206 -0.09469 -0.09301 -0.08666 -0.07783 -0.10395 -0.11801 -0.11932 -0.13966 -0.12230 -0.12342 -0.15560 -0.16403 -0.17900 -0.20310 -0.25196 -0.27828 -0.29024 -0.33610 -0.31562 -0.33692 -0.25907 -0.24879 -0.20542 -0.30642 -0.23422 -0.24080 -0.23805 -0.24764 -0.16755 -0.31484 -0.21897 -0.31486 -0.25441 -0.31849 -0.31480 -0.31527 -0.28724 -0.26799 -0.22513 -0.19348 -0.20415 -0.25581 -0.26543 -0.27212 -0.24278 -0.27217 -0.26717 -0.23336 -0.27033 -0.25019 -0.23787 -0.18458 -0.21078 -0.18628 -0.22139 -0.21083 -0.21548 -0.22256 -0.23717 -0.24373 -0.22250 -0.17765 -0.13893 -0.13893 -0.15496 -0.15951 -0.18788 -0.18331 -0.19824 -0.16923 -0.17186 -0.19059 -0.22090 -0.22686 -0.28440 -0.25613 -0.27206 -0.23698 -0.22436 -0.23689 -0.23894 -0.28381 -0.29970 -0.32952 -0.34526 -0.29413 -0.24753 -0.22536 -0.24983 -0.29188 -0.28425 -0.28328 -0.22254 -0.19942 -0.16942 -0.19071 -0.22117 -0.18794 -0.20649 -0.19079 -0.16780 -0.17370 -0.16425 -0.15571 -0.14304 -0.12140 -0.10847 -0.13202 -0.15789 -0.15211 -0.13961 -0.13412 -0.11919 -0.14426 -0.13587 -0.13357 -0.12791 -0.12565 -0.10807 -0.09930 -0.09386 -0.11263 -0.12113 -0.13815 -0.14321 -0.16193 -0.17177 -0.14212 -0.12620 -0.11934 -0.11290 -0.11627 -0.12969 -0.14770 -0.14652 -0.16101 -0.18403 -0.16904 -0.15928 -0.16996 -0.12972 -0.13802 -0.12005 -0.13097 -0.13649 -0.11978 -0.11253 -0.10597 -0.11091 -0.11033 -0.10910 -0.10160 -0.10123 -0.10298 -0.10583 -0.09811 -0.09156 -0.09240 -0.09924 -0.09970 -0.09614 -0.09953 -0.09390 -0.09133 -0.08979 -0.09443 -0.09376 -0.09950 -0.09751 -0.10522 -0.12085 -0.11736 -0.12292 -0.13317 -0.12567 -0.13654 -0.12618 -0.11773 -0.11673 -0.11780 -0.12382 -0.11912 -0.13101 -0.12548 -0.13375 -0.13813 -0.15071 -0.13651 -0.14172 -0.14765 -0.14220 -0.13573 -0.11923 -0.12373 -0.13875 -0.13037 -0.15494 -0.14163 -0.14531 -0.14212 -0.12736 -0.12497 -0.10931 -0.09926 -0.05894 -0.04269 -0.02581 -2.04e-03 -0.02671 0.01968 -0.03927 -0.11375 -0.10334 -0.12465 -0.14743 -0.14967 -0.13906 -0.11129 -0.11235 -0.10955 -0.09881 -0.10040 -0.07057 -0.02077 -9.13e-03 -0.02596 -0.04497 -0.05721 0.03156 0.02850 -0.05257 -0.04090 -0.07030 -0.02676 -0.04100 8.51e-03 2.24e-04 -0.05238 -0.05489 -0.14500 -0.10393 -0.10164 -0.03653 -0.08814 -0.12957 -0.14558 -0.13158 -0.15670 -0.15367 -0.14815 -0.14422 -0.13516 -0.12061 -0.12110 -0.07824 -0.07933 -0.01294 6.16e-03 0.01273 -0.09188 -0.05016 -0.06141 0.05084 0.02692 0.02881 -0.02692 -0.04946 -0.09568 -0.10906 -0.08371 -0.11179 -0.10888 -0.10170 -0.09048 -0.07550 -0.08042 -0.07430 -0.07728 -0.07378 -0.02014 0.05129 -0.06523 -0.10861 -0.09347 -0.08750 -0.06036 -0.01243 0.01437 2.34e-03 -1.88e-03 -0.04813 -0.04296 -0.07711 -0.06025 -0.06574 -0.10476 -0.11079 -0.12548 -0.11303 -0.10668 -0.09430 -0.12501 -0.11089 -0.12555 -0.12131 -0.11829 -0.06574 -0.08763 -0.04695 -0.09618 -0.05458 -0.05742 -0.04818 -0.03951 0.02643 0.10353 0.12197 0.24953 0.14606 0.21858 0.16028 0.21780 0.19999 0.06692 0.09833 0.09287 0.17018 0.10774 0.22905 0.22816 0.26747 0.17504 0.23662 0.23097 0.18574 0.13375 0.07835 0.05417 0.08113 0.06532 0.07492 0.03228 -0.19631 -0.09504 -0.07014 -0.01836 -0.19376 -0.15046 -0.24297 -0.21364 -0.26679 -0.26302 -0.28448 -0.25962 -0.25281 -0.21648 -0.21549 -0.21998 -0.26732 -0.26340 -0.26590 -0.24902 -0.26692 -0.24837 -0.24823 -0.24818 -0.23184 -0.25104 -0.23687 -0.20344 -0.19670 -0.14577 -0.12912 -0.08645 -0.04075 -0.01046 -0.01266 -6.15e-03 0.01121 8.03e-03 5.31e-03 -0.01190 -5.22e-03 -8.26e-04 2.29e-03 6.21e-03 4.39e-04 7.14e-03 6.02e-03 0.01608 0.01257 0.01106 0.01359 5.26e-03 2.83e-03 0.01429 3.43e-03 -0.02494 -0.11889 -0.21211 -0.17674 -0.18685 -0.20712 -0.22221 -0.19496 -0.18219 -0.16759 -0.14235 -0.13876 -0.16096 -0.14747 -0.15912 -0.18675 -0.18461 -0.18789 -0.19592 -0.20266 -0.20468 -0.20142 -0.18675 -0.22216 -0.20061 -0.20047 -0.19799 -0.17476 -0.17754 -0.18515 -0.19817 -0.18964 -0.21002 -0.21943 -0.19871 -0.20396 -0.19776 -0.21884 -0.25046 -0.25589 -0.27602 -0.27051 -0.27789 -0.27678 -0.23028 -0.22764 -0.21955 -0.21001 -0.22550 -0.25381 -0.25943 -0.22142 -0.19288 -0.19552 -0.23800 -0.20451 -0.21355 -0.17454 -0.11622 -0.10425 -0.02415 -0.10679 -0.14636 -0.19367 -0.09151 -0.12104 -0.06877 -0.13256 -0.08316 -0.05165 0.01140 -9.48e-03 0.02467 0.01525 -0.02266 -7.35e-03 5.26e-03 0.03432 0.02604 0.02501 -0.03582 -0.04544 -0.11687 -0.07871 -0.07815 -0.06481 -0.06157 -0.07612 -0.07264 -0.07632 -0.08048 -0.07472 -0.12085 -0.09617 -0.15608 -0.14028 -0.13534 -0.12912 -0.11270 -0.11413 -0.10163 -0.07038 -0.13040 -0.15175 -0.14938 -0.13624 -0.12437 -0.12231 -0.10512 -0.09801 -0.09507 -0.09678 -0.09215 -0.07508 -0.05659 -0.05484 -0.06264 -0.07909 -0.08608 -0.08861 -0.09177 -0.08217 -0.07122 -0.08803 -0.08411 -0.08964 -0.08410 -0.09307 -0.08595 -0.10700 -0.10991 -0.12035 -0.11374 -0.11536 -0.11955 -0.09977 -0.09545 -0.07767 -0.07479 -0.06459 -0.05243 -0.05476 -0.05628 -0.06754 -0.07109 -0.06780 -0.07800 -0.06557 -0.06917 -0.07323 -0.07204 -0.07442 -0.07529 -0.06581 -0.06384 -0.08744 -0.10276 -0.09867 -0.10865 -0.12294 -0.12888 -0.10241 -0.09630 -0.10826 -0.13090 -0.13999 -0.14994 -0.12918 -0.14977 -0.14188 -0.16125 -0.16435 -0.14587 -0.13571 -0.13586 -0.11712 -0.10275 -0.09929 -0.08472 -0.09293 -0.08486 -0.07446 -0.07700 -0.05884 -0.04414 -0.03847 -0.05548 -0.08085 -0.09066 -0.09031 -0.08162 -0.07842 -0.07024 -0.07763 -0.08264 -0.08557 -0.08885 -0.08732 -0.08866 -0.09134 -0.08704 -0.08471 -0.07976 -0.09389 -0.09127 -0.08914 -0.08872 -0.08706 -0.07180 -0.08256 -0.07608 -0.07295 -0.06743 -0.07307 -0.07053 -0.05082 -0.03798 -0.03571 -0.03493 -0.03595 -0.02416 -0.02887 -0.02843 -0.02701 -0.05883 -0.04628 -0.03604 -0.04167 -0.07571 -0.06790 -0.06174 -0.05879 -0.06949 -0.07459 -0.06091 -0.06181 -0.06946 -0.04416 -0.02091 -0.01793 -0.01464 -0.08453 -0.06450 -0.06664 -0.06973 -0.06699 -0.06406 -0.06873 -0.03257 -0.03945 -0.03052 -0.03158 -0.02617 -0.01700 -0.03397 -0.02936 -0.07489 -0.07390 -0.07588 -0.03891 -0.02788 -0.02839 -0.02707 -0.03330 -0.07836 -0.07528 -0.06856 -0.06722 -0.05247 -0.04758 -0.04385 -0.04201 -0.03656 -0.03051 -0.03981 -0.04001 -0.02067 -0.01990 -0.02600 -0.01722 -0.04040 -0.12004 -0.13970 -0.17256 -0.17397 -0.18614 -0.20930 -0.21500 -0.23691 -0.22669 -0.22687 -0.22805 -0.23156 -0.20501 -0.17858 -0.13677 -0.10626 -0.04475 -0.16951 -0.28751 -0.28298 -0.33466 -0.35308 -0.36036 -0.34900 -0.33648 -0.33233 -0.31891 -0.33602 -0.29271 -0.30841 -0.30705 -0.29241 -0.29454 -0.30228 -0.29214 -0.23737 -0.22790 -0.23550 -0.23552 -0.22697 -0.21182 -0.27285 -0.26227 -0.33232 -0.31886 -0.29225 -0.28778 -0.29967 -0.28761 -0.32674 -0.34162 -0.33113 -0.26894 -0.23825 -0.24049 -0.24553 -0.24888 -0.27612 -0.26568 -0.27739 -0.26126 -0.25677 -0.28314 -0.28091 -0.31292 -0.33661 -0.34658 -0.36053 -0.38754 -0.39832 -0.36143 -0.39784 -0.35608, \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 5e4+1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(),\"savee/model_anaconda3\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, cmap, max_V)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# U_NN_min = U_NN.min()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# U_NN  = U_NN-U_NN_min\u001b[39;00m\n\u001b[1;32m     21\u001b[0m fig, ax    \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m),dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,constrained_layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m c        \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/axes/_base.py:489\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    488\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 489\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m index_of(xy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/matplotlib/cbook.py:1358\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for choose_id in [1,2,3]:\n",
    "model_U = FCNN(input_dim=dim,output_dim=1,num_hidden=3,hidden_dim=10,act=tanh)#.cuda()\n",
    "model   = Model(dim,model_U=model_U)#.cuda();\n",
    "SOL     = Solver(model)\n",
    "\n",
    "# print(model.mu,model.sigma,model.coef_U)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.001))\n",
    "scheduler = None\n",
    "_loss_step = SOL.train_model(data_train=X,data_test=X,\n",
    "                             get_loss=get_loss,optimizer=optimizer,scheduler=scheduler,\n",
    "                             n_steps=int(1e1+1),batch_size=500,n_show_loss=1000,use_tqdm=True)\n",
    "\n",
    "# 5e4+1\n",
    "torch.cuda.empty_cache()\n",
    "plot_model(model)\n",
    "# torch.save(model.state_dict(), \"savee/model_\"+str(choose_id))\n",
    "#torch.save(model.state_dict(),\"savee/model_anaconda3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f2b8b6-9f6d-4fae-ae29-87c4a6cb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"savee/model_99\")\n",
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b07f7a-fb98-4405-8790-18bc4afbc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "# xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "# U_NN     = model.get_U_np(xx)\n",
    "# U_NN_min = U_NN.min()\n",
    "# U_NN     = U_NN-U_NN_min\n",
    "# c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5)\n",
    "# ax.legend(fontsize=10)\n",
    "# ax.set_xlabel('$x$',fontsize=10)\n",
    "# ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "# ax.set_xlim([0,2])\n",
    "# ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "# ax.set_xticks([0,.5,1.,1.5,2])\n",
    "# ax.yaxis.grid(linestyle='--')\n",
    "# ax.tick_params(axis=\"both\", labelsize=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc4b4d82-80a8-422e-a7d9-d2623106808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839674fe",
   "metadata": {},
   "source": [
    "# Visualizing the results for different a_k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344c6cc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_models(models):\n",
    "    \n",
    "    xx       = np.linspace(0,2,1000).reshape(-1,1)\n",
    "    fig, ax  = plt.subplots(1,1,figsize=(4,3),dpi=200,constrained_layout=True)\n",
    "    \n",
    "    for k,model_name in enumerate(models):\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        U_NN     = model.get_U_np(xx)\n",
    "        U_NN_min = U_NN.min()\n",
    "        U_NN     = U_NN-U_NN_min\n",
    "        c        = ax.plot(xx[:,0],U_NN,'-',lw=1.5,label=\"$a_{%d}(x)$\"%(k+1))\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlabel('$x$',fontsize=10)\n",
    "    ax.set_ylabel('$U(x)$',fontsize=10)\n",
    "    ax.set_xlim([0,2])\n",
    "    ax.set_yticks([0,0.2,0.4,0.6,0.8,1,1.2])\n",
    "    ax.set_xticks([0,.5,1.,1.5,2])\n",
    "    ax.yaxis.grid(linestyle='--')\n",
    "    ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_models([\"savee/model_anaconda3\"])\n",
    "#plot_models([\"savee/model_a1_update\", \"savee/model_a2_update\", \"savee/model_a3_update\", \"savee/model_a4\", \"savee/model_a5_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0851750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b79c6ed-2669-4f78-a26d-a70b834146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"/Users/annacoletti/Desktop/savee/model_a3_update.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "319e3d20-3380-447c-9b87-866c7b207395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as scio\n",
    "# from scipy.io import savemat\n",
    "# scio.savemat('W2_learned_DL', U_NN)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
